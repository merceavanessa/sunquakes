{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from a supcon ft checkpoint\n",
    "features = torch.tensor([[[ 0.1896,  0.1470, -0.0056, -0.1029,  0.0668,  0.0709,  0.1129,\n",
    "           0.0326,  0.0249,  0.1569, -0.1531, -0.0543,  0.1183,  0.0820,\n",
    "          -0.0224,  0.0829, -0.0474, -0.0741,  0.1123, -0.1090, -0.0932,\n",
    "           0.1561, -0.1592,  0.0865,  0.0084, -0.0658,  0.0077, -0.0099,\n",
    "          -0.0108, -0.0451,  0.0303, -0.0391, -0.0061, -0.0817, -0.0577,\n",
    "          -0.1445, -0.0512, -0.0398, -0.0066, -0.0143, -0.0672, -0.0898,\n",
    "           0.1030, -0.1166, -0.1842, -0.0562,  0.0200,  0.1398, -0.0084,\n",
    "          -0.0451, -0.0318,  0.0233, -0.0802,  0.0635,  0.0939, -0.1296,\n",
    "          -0.0895,  0.0440, -0.0484,  0.0898,  0.2157,  0.0965,  0.0071,\n",
    "           0.0365,  0.0056,  0.1206,  0.0026,  0.1124, -0.1264,  0.0144,\n",
    "          -0.1464, -0.0801,  0.0229, -0.0759, -0.0019, -0.0011,  0.0376,\n",
    "           0.1132, -0.0075,  0.1261,  0.2407,  0.1217, -0.0642, -0.1025,\n",
    "          -0.1382, -0.0627, -0.0248, -0.0127, -0.0251,  0.0279,  0.1106,\n",
    "          -0.0484,  0.0828, -0.0387, -0.0137,  0.0944, -0.1367, -0.1341,\n",
    "           0.0172, -0.0854, -0.0451, -0.0162,  0.0110,  0.0243, -0.0916,\n",
    "          -0.1275, -0.0163,  0.1123, -0.0775, -0.1367, -0.0491,  0.0157,\n",
    "          -0.0882, -0.0057,  0.1832, -0.0631, -0.0298,  0.0025, -0.0190,\n",
    "           0.1001, -0.1523, -0.1408,  0.0507, -0.0469,  0.0361, -0.0078,\n",
    "          -0.0096, -0.0685],\n",
    "         [ 0.1895,  0.1468, -0.0058, -0.1031,  0.0670,  0.0707,  0.1129,\n",
    "           0.0329,  0.0250,  0.1564, -0.1530, -0.0544,  0.1183,  0.0820,\n",
    "          -0.0222,  0.0832, -0.0476, -0.0740,  0.1121, -0.1093, -0.0932,\n",
    "           0.1561, -0.1590,  0.0867,  0.0082, -0.0658,  0.0077, -0.0097,\n",
    "          -0.0108, -0.0448,  0.0302, -0.0389, -0.0061, -0.0816, -0.0577,\n",
    "          -0.1443, -0.0512, -0.0397, -0.0065, -0.0147, -0.0673, -0.0898,\n",
    "           0.1032, -0.1164, -0.1839, -0.0567,  0.0200,  0.1401, -0.0083,\n",
    "          -0.0451, -0.0317,  0.0231, -0.0802,  0.0635,  0.0939, -0.1295,\n",
    "          -0.0893,  0.0443, -0.0485,  0.0899,  0.2155,  0.0968,  0.0073,\n",
    "           0.0366,  0.0057,  0.1207,  0.0026,  0.1126, -0.1263,  0.0141,\n",
    "          -0.1465, -0.0798,  0.0233, -0.0759, -0.0023, -0.0013,  0.0377,\n",
    "           0.1130, -0.0076,  0.1261,  0.2408,  0.1215, -0.0643, -0.1024,\n",
    "          -0.1381, -0.0628, -0.0250, -0.0123, -0.0252,  0.0277,  0.1109,\n",
    "          -0.0484,  0.0829, -0.0388, -0.0137,  0.0944, -0.1369, -0.1345,\n",
    "           0.0170, -0.0853, -0.0450, -0.0162,  0.0108,  0.0247, -0.0915,\n",
    "          -0.1275, -0.0165,  0.1124, -0.0777, -0.1367, -0.0489,  0.0155,\n",
    "          -0.0883, -0.0058,  0.1832, -0.0633, -0.0298,  0.0025, -0.0190,\n",
    "           0.1002, -0.1523, -0.1407,  0.0508, -0.0469,  0.0362, -0.0079,\n",
    "          -0.0097, -0.0684]],\n",
    "\n",
    "        [[ 0.1887,  0.1448, -0.0085, -0.1055,  0.0686,  0.0683,  0.1125,\n",
    "           0.0353,  0.0253,  0.1513, -0.1521, -0.0550,  0.1185,  0.0819,\n",
    "          -0.0194,  0.0862, -0.0502, -0.0727,  0.1106, -0.1122, -0.0925,\n",
    "           0.1563, -0.1572,  0.0890,  0.0068, -0.0665,  0.0079, -0.0070,\n",
    "          -0.0110, -0.0418,  0.0292, -0.0371, -0.0059, -0.0810, -0.0571,\n",
    "          -0.1419, -0.0508, -0.0388, -0.0055, -0.0191, -0.0690, -0.0899,\n",
    "           0.1054, -0.1136, -0.1806, -0.0609,  0.0194,  0.1428, -0.0076,\n",
    "          -0.0446, -0.0306,  0.0208, -0.0804,  0.0639,  0.0937, -0.1293,\n",
    "          -0.0873,  0.0469, -0.0486,  0.0906,  0.2132,  0.1005,  0.0095,\n",
    "           0.0386,  0.0059,  0.1210,  0.0033,  0.1147, -0.1257,  0.0107,\n",
    "          -0.1472, -0.0768,  0.0268, -0.0755, -0.0061, -0.0038,  0.0382,\n",
    "           0.1109, -0.0089,  0.1270,  0.2413,  0.1194, -0.0654, -0.1018,\n",
    "          -0.1371, -0.0639, -0.0267, -0.0085, -0.0258,  0.0264,  0.1136,\n",
    "          -0.0483,  0.0838, -0.0395, -0.0134,  0.0945, -0.1392, -0.1383,\n",
    "           0.0151, -0.0841, -0.0447, -0.0153,  0.0088,  0.0283, -0.0909,\n",
    "          -0.1275, -0.0184,  0.1135, -0.0799, -0.1371, -0.0477,  0.0138,\n",
    "          -0.0887, -0.0064,  0.1831, -0.0654, -0.0293,  0.0027, -0.0192,\n",
    "           0.1016, -0.1528, -0.1391,  0.0523, -0.0477,  0.0377, -0.0084,\n",
    "          -0.0108, -0.0679],\n",
    "         [ 0.1925,  0.1672,  0.0299, -0.0673,  0.0424,  0.0990,  0.1141,\n",
    "          -0.0004,  0.0194,  0.2180, -0.1584, -0.0438,  0.1109,  0.0803,\n",
    "          -0.0580,  0.0405, -0.0115, -0.0884,  0.1278, -0.0658, -0.0982,\n",
    "           0.1466, -0.1763,  0.0530,  0.0265, -0.0547,  0.0049, -0.0440,\n",
    "          -0.0077, -0.0826,  0.0423, -0.0614, -0.0090, -0.0870, -0.0633,\n",
    "          -0.1692, -0.0541, -0.0509, -0.0194,  0.0441, -0.0428, -0.0849,\n",
    "           0.0711, -0.1484, -0.2199,  0.0014,  0.0260,  0.0984, -0.0173,\n",
    "          -0.0502, -0.0451,  0.0527, -0.0739,  0.0552,  0.0917, -0.1275,\n",
    "          -0.1124,  0.0073, -0.0446,  0.0763,  0.2372,  0.0447, -0.0214,\n",
    "           0.0093,  0.0023,  0.1114, -0.0064,  0.0802, -0.1293,  0.0580,\n",
    "          -0.1307, -0.1151, -0.0245, -0.0773,  0.0475,  0.0324,  0.0294,\n",
    "           0.1361,  0.0105,  0.1103,  0.2233,  0.1445, -0.0474, -0.1057,\n",
    "          -0.1455, -0.0463, -0.0021, -0.0617, -0.0155,  0.0448,  0.0699,\n",
    "          -0.0474,  0.0668, -0.0275, -0.0170,  0.0895, -0.1005, -0.0790,\n",
    "           0.0420, -0.0972, -0.0482, -0.0263,  0.0372, -0.0246, -0.0966,\n",
    "          -0.1224,  0.0093,  0.0925, -0.0466, -0.1259, -0.0638,  0.0369,\n",
    "          -0.0793,  0.0033,  0.1770, -0.0331, -0.0342,  0.0004, -0.0159,\n",
    "           0.0781, -0.1401, -0.1551,  0.0294, -0.0350,  0.0149, -0.0003,\n",
    "           0.0046, -0.0723]],\n",
    "\n",
    "        [[ 0.1904,  0.1491, -0.0027, -0.1002,  0.0650,  0.0735,  0.1134,\n",
    "           0.0300,  0.0245,  0.1624, -0.1540, -0.0536,  0.1180,  0.0821,\n",
    "          -0.0254,  0.0797, -0.0445, -0.0755,  0.1139, -0.1057, -0.0939,\n",
    "           0.1557, -0.1611,  0.0839,  0.0099, -0.0650,  0.0075, -0.0128,\n",
    "          -0.0106, -0.0483,  0.0314, -0.0411, -0.0064, -0.0824, -0.0584,\n",
    "          -0.1469, -0.0516, -0.0409, -0.0077, -0.0095, -0.0653, -0.0897,\n",
    "           0.1007, -0.1196, -0.1877, -0.0516,  0.0206,  0.1368, -0.0092,\n",
    "          -0.0457, -0.0330,  0.0258, -0.0799,  0.0630,  0.0940, -0.1298,\n",
    "          -0.0916,  0.0411, -0.0483,  0.0890,  0.2181,  0.0924,  0.0048,\n",
    "           0.0343,  0.0054,  0.1202,  0.0018,  0.1101, -0.1270,  0.0180,\n",
    "          -0.1456, -0.0832,  0.0191, -0.0762,  0.0022,  0.0017,  0.0370,\n",
    "           0.1154, -0.0060,  0.1251,  0.2399,  0.1239, -0.0630, -0.1030,\n",
    "          -0.1392, -0.0615, -0.0230, -0.0168, -0.0244,  0.0294,  0.1075,\n",
    "          -0.0484,  0.0817, -0.0379, -0.0140,  0.0943, -0.1341, -0.1300,\n",
    "           0.0193, -0.0866, -0.0455, -0.0171,  0.0132,  0.0203, -0.0923,\n",
    "          -0.1275, -0.0142,  0.1109, -0.0752, -0.1362, -0.0504,  0.0175,\n",
    "          -0.0877, -0.0050,  0.1832, -0.0608, -0.0303,  0.0023, -0.0188,\n",
    "           0.0985, -0.1517, -0.1424,  0.0491, -0.0460,  0.0344, -0.0072,\n",
    "          -0.0085, -0.0690],\n",
    "         [ 0.1916,  0.1529,  0.0029, -0.0950,  0.0614,  0.0782,  0.1140,\n",
    "           0.0249,  0.0238,  0.1727, -0.1554, -0.0522,  0.1174,  0.0822,\n",
    "          -0.0311,  0.0733, -0.0391, -0.0780,  0.1168, -0.0993, -0.0951,\n",
    "           0.1549, -0.1644,  0.0790,  0.0128, -0.0635,  0.0071, -0.0182,\n",
    "          -0.0101, -0.0544,  0.0334, -0.0447, -0.0069, -0.0835, -0.0595,\n",
    "          -0.1514, -0.0523, -0.0428, -0.0097, -0.0003, -0.0618, -0.0892,\n",
    "           0.0961, -0.1251, -0.1940, -0.0428,  0.0216,  0.1308, -0.0106,\n",
    "          -0.0467, -0.0352,  0.0306, -0.0792,  0.0619,  0.0940, -0.1300,\n",
    "          -0.0956,  0.0355, -0.0479,  0.0872,  0.2224,  0.0847,  0.0003,\n",
    "           0.0302,  0.0049,  0.1193,  0.0004,  0.1054, -0.1280,  0.0250,\n",
    "          -0.1437, -0.0890,  0.0117, -0.0768,  0.0099,  0.0070,  0.0359,\n",
    "           0.1195, -0.0032,  0.1232,  0.2382,  0.1280, -0.0606, -0.1039,\n",
    "          -0.1409, -0.0592, -0.0195, -0.0245, -0.0230,  0.0321,  0.1016,\n",
    "          -0.0485,  0.0795, -0.0363, -0.0146,  0.0939, -0.1289, -0.1218,\n",
    "           0.0233, -0.0888, -0.0461, -0.0188,  0.0174,  0.0127, -0.0934,\n",
    "          -0.1272, -0.0103,  0.1083, -0.0706, -0.1350, -0.0529,  0.0209,\n",
    "          -0.0867, -0.0036,  0.1830, -0.0564, -0.0311,  0.0020, -0.0184,\n",
    "           0.0955, -0.1504, -0.1452,  0.0459, -0.0443,  0.0312, -0.0061,\n",
    "          -0.0063, -0.0699]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "features.shape\n",
    "labels = torch.tensor([[0,0,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.tensor([[1,0,1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contrast_count = 2 # look at features for two images at once (anchor + \"positive\")\n",
    "temperature = 0.1\n",
    "# mask = torch.eye(batch_size, dtype=torch.float32) # simclr\n",
    "mask = torch.eq(labels, labels.T).float()\n",
    "\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 128]), torch.Size([3, 128]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1, f2 = torch.unbind(features, dim=1)\n",
    "f1.shape, f2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1896,  0.1470, -0.0056, -0.1029,  0.0668,  0.0709,  0.1129,  0.0326,\n",
       "          0.0249,  0.1569, -0.1531, -0.0543,  0.1183,  0.0820, -0.0224,  0.0829,\n",
       "         -0.0474, -0.0741,  0.1123, -0.1090, -0.0932,  0.1561, -0.1592,  0.0865,\n",
       "          0.0084, -0.0658,  0.0077, -0.0099, -0.0108, -0.0451,  0.0303, -0.0391,\n",
       "         -0.0061, -0.0817, -0.0577, -0.1445, -0.0512, -0.0398, -0.0066, -0.0143,\n",
       "         -0.0672, -0.0898,  0.1030, -0.1166, -0.1842, -0.0562,  0.0200,  0.1398,\n",
       "         -0.0084, -0.0451, -0.0318,  0.0233, -0.0802,  0.0635,  0.0939, -0.1296,\n",
       "         -0.0895,  0.0440, -0.0484,  0.0898,  0.2157,  0.0965,  0.0071,  0.0365,\n",
       "          0.0056,  0.1206,  0.0026,  0.1124, -0.1264,  0.0144, -0.1464, -0.0801,\n",
       "          0.0229, -0.0759, -0.0019, -0.0011,  0.0376,  0.1132, -0.0075,  0.1261,\n",
       "          0.2407,  0.1217, -0.0642, -0.1025, -0.1382, -0.0627, -0.0248, -0.0127,\n",
       "         -0.0251,  0.0279,  0.1106, -0.0484,  0.0828, -0.0387, -0.0137,  0.0944,\n",
       "         -0.1367, -0.1341,  0.0172, -0.0854, -0.0451, -0.0162,  0.0110,  0.0243,\n",
       "         -0.0916, -0.1275, -0.0163,  0.1123, -0.0775, -0.1367, -0.0491,  0.0157,\n",
       "         -0.0882, -0.0057,  0.1832, -0.0631, -0.0298,  0.0025, -0.0190,  0.1001,\n",
       "         -0.1523, -0.1408,  0.0507, -0.0469,  0.0361, -0.0078, -0.0096, -0.0685],\n",
       "        [ 0.1887,  0.1448, -0.0085, -0.1055,  0.0686,  0.0683,  0.1125,  0.0353,\n",
       "          0.0253,  0.1513, -0.1521, -0.0550,  0.1185,  0.0819, -0.0194,  0.0862,\n",
       "         -0.0502, -0.0727,  0.1106, -0.1122, -0.0925,  0.1563, -0.1572,  0.0890,\n",
       "          0.0068, -0.0665,  0.0079, -0.0070, -0.0110, -0.0418,  0.0292, -0.0371,\n",
       "         -0.0059, -0.0810, -0.0571, -0.1419, -0.0508, -0.0388, -0.0055, -0.0191,\n",
       "         -0.0690, -0.0899,  0.1054, -0.1136, -0.1806, -0.0609,  0.0194,  0.1428,\n",
       "         -0.0076, -0.0446, -0.0306,  0.0208, -0.0804,  0.0639,  0.0937, -0.1293,\n",
       "         -0.0873,  0.0469, -0.0486,  0.0906,  0.2132,  0.1005,  0.0095,  0.0386,\n",
       "          0.0059,  0.1210,  0.0033,  0.1147, -0.1257,  0.0107, -0.1472, -0.0768,\n",
       "          0.0268, -0.0755, -0.0061, -0.0038,  0.0382,  0.1109, -0.0089,  0.1270,\n",
       "          0.2413,  0.1194, -0.0654, -0.1018, -0.1371, -0.0639, -0.0267, -0.0085,\n",
       "         -0.0258,  0.0264,  0.1136, -0.0483,  0.0838, -0.0395, -0.0134,  0.0945,\n",
       "         -0.1392, -0.1383,  0.0151, -0.0841, -0.0447, -0.0153,  0.0088,  0.0283,\n",
       "         -0.0909, -0.1275, -0.0184,  0.1135, -0.0799, -0.1371, -0.0477,  0.0138,\n",
       "         -0.0887, -0.0064,  0.1831, -0.0654, -0.0293,  0.0027, -0.0192,  0.1016,\n",
       "         -0.1528, -0.1391,  0.0523, -0.0477,  0.0377, -0.0084, -0.0108, -0.0679],\n",
       "        [ 0.1904,  0.1491, -0.0027, -0.1002,  0.0650,  0.0735,  0.1134,  0.0300,\n",
       "          0.0245,  0.1624, -0.1540, -0.0536,  0.1180,  0.0821, -0.0254,  0.0797,\n",
       "         -0.0445, -0.0755,  0.1139, -0.1057, -0.0939,  0.1557, -0.1611,  0.0839,\n",
       "          0.0099, -0.0650,  0.0075, -0.0128, -0.0106, -0.0483,  0.0314, -0.0411,\n",
       "         -0.0064, -0.0824, -0.0584, -0.1469, -0.0516, -0.0409, -0.0077, -0.0095,\n",
       "         -0.0653, -0.0897,  0.1007, -0.1196, -0.1877, -0.0516,  0.0206,  0.1368,\n",
       "         -0.0092, -0.0457, -0.0330,  0.0258, -0.0799,  0.0630,  0.0940, -0.1298,\n",
       "         -0.0916,  0.0411, -0.0483,  0.0890,  0.2181,  0.0924,  0.0048,  0.0343,\n",
       "          0.0054,  0.1202,  0.0018,  0.1101, -0.1270,  0.0180, -0.1456, -0.0832,\n",
       "          0.0191, -0.0762,  0.0022,  0.0017,  0.0370,  0.1154, -0.0060,  0.1251,\n",
       "          0.2399,  0.1239, -0.0630, -0.1030, -0.1392, -0.0615, -0.0230, -0.0168,\n",
       "         -0.0244,  0.0294,  0.1075, -0.0484,  0.0817, -0.0379, -0.0140,  0.0943,\n",
       "         -0.1341, -0.1300,  0.0193, -0.0866, -0.0455, -0.0171,  0.0132,  0.0203,\n",
       "         -0.0923, -0.1275, -0.0142,  0.1109, -0.0752, -0.1362, -0.0504,  0.0175,\n",
       "         -0.0877, -0.0050,  0.1832, -0.0608, -0.0303,  0.0023, -0.0188,  0.0985,\n",
       "         -0.1517, -0.1424,  0.0491, -0.0460,  0.0344, -0.0072, -0.0085, -0.0690],\n",
       "        [ 0.1895,  0.1468, -0.0058, -0.1031,  0.0670,  0.0707,  0.1129,  0.0329,\n",
       "          0.0250,  0.1564, -0.1530, -0.0544,  0.1183,  0.0820, -0.0222,  0.0832,\n",
       "         -0.0476, -0.0740,  0.1121, -0.1093, -0.0932,  0.1561, -0.1590,  0.0867,\n",
       "          0.0082, -0.0658,  0.0077, -0.0097, -0.0108, -0.0448,  0.0302, -0.0389,\n",
       "         -0.0061, -0.0816, -0.0577, -0.1443, -0.0512, -0.0397, -0.0065, -0.0147,\n",
       "         -0.0673, -0.0898,  0.1032, -0.1164, -0.1839, -0.0567,  0.0200,  0.1401,\n",
       "         -0.0083, -0.0451, -0.0317,  0.0231, -0.0802,  0.0635,  0.0939, -0.1295,\n",
       "         -0.0893,  0.0443, -0.0485,  0.0899,  0.2155,  0.0968,  0.0073,  0.0366,\n",
       "          0.0057,  0.1207,  0.0026,  0.1126, -0.1263,  0.0141, -0.1465, -0.0798,\n",
       "          0.0233, -0.0759, -0.0023, -0.0013,  0.0377,  0.1130, -0.0076,  0.1261,\n",
       "          0.2408,  0.1215, -0.0643, -0.1024, -0.1381, -0.0628, -0.0250, -0.0123,\n",
       "         -0.0252,  0.0277,  0.1109, -0.0484,  0.0829, -0.0388, -0.0137,  0.0944,\n",
       "         -0.1369, -0.1345,  0.0170, -0.0853, -0.0450, -0.0162,  0.0108,  0.0247,\n",
       "         -0.0915, -0.1275, -0.0165,  0.1124, -0.0777, -0.1367, -0.0489,  0.0155,\n",
       "         -0.0883, -0.0058,  0.1832, -0.0633, -0.0298,  0.0025, -0.0190,  0.1002,\n",
       "         -0.1523, -0.1407,  0.0508, -0.0469,  0.0362, -0.0079, -0.0097, -0.0684],\n",
       "        [ 0.1925,  0.1672,  0.0299, -0.0673,  0.0424,  0.0990,  0.1141, -0.0004,\n",
       "          0.0194,  0.2180, -0.1584, -0.0438,  0.1109,  0.0803, -0.0580,  0.0405,\n",
       "         -0.0115, -0.0884,  0.1278, -0.0658, -0.0982,  0.1466, -0.1763,  0.0530,\n",
       "          0.0265, -0.0547,  0.0049, -0.0440, -0.0077, -0.0826,  0.0423, -0.0614,\n",
       "         -0.0090, -0.0870, -0.0633, -0.1692, -0.0541, -0.0509, -0.0194,  0.0441,\n",
       "         -0.0428, -0.0849,  0.0711, -0.1484, -0.2199,  0.0014,  0.0260,  0.0984,\n",
       "         -0.0173, -0.0502, -0.0451,  0.0527, -0.0739,  0.0552,  0.0917, -0.1275,\n",
       "         -0.1124,  0.0073, -0.0446,  0.0763,  0.2372,  0.0447, -0.0214,  0.0093,\n",
       "          0.0023,  0.1114, -0.0064,  0.0802, -0.1293,  0.0580, -0.1307, -0.1151,\n",
       "         -0.0245, -0.0773,  0.0475,  0.0324,  0.0294,  0.1361,  0.0105,  0.1103,\n",
       "          0.2233,  0.1445, -0.0474, -0.1057, -0.1455, -0.0463, -0.0021, -0.0617,\n",
       "         -0.0155,  0.0448,  0.0699, -0.0474,  0.0668, -0.0275, -0.0170,  0.0895,\n",
       "         -0.1005, -0.0790,  0.0420, -0.0972, -0.0482, -0.0263,  0.0372, -0.0246,\n",
       "         -0.0966, -0.1224,  0.0093,  0.0925, -0.0466, -0.1259, -0.0638,  0.0369,\n",
       "         -0.0793,  0.0033,  0.1770, -0.0331, -0.0342,  0.0004, -0.0159,  0.0781,\n",
       "         -0.1401, -0.1551,  0.0294, -0.0350,  0.0149, -0.0003,  0.0046, -0.0723],\n",
       "        [ 0.1916,  0.1529,  0.0029, -0.0950,  0.0614,  0.0782,  0.1140,  0.0249,\n",
       "          0.0238,  0.1727, -0.1554, -0.0522,  0.1174,  0.0822, -0.0311,  0.0733,\n",
       "         -0.0391, -0.0780,  0.1168, -0.0993, -0.0951,  0.1549, -0.1644,  0.0790,\n",
       "          0.0128, -0.0635,  0.0071, -0.0182, -0.0101, -0.0544,  0.0334, -0.0447,\n",
       "         -0.0069, -0.0835, -0.0595, -0.1514, -0.0523, -0.0428, -0.0097, -0.0003,\n",
       "         -0.0618, -0.0892,  0.0961, -0.1251, -0.1940, -0.0428,  0.0216,  0.1308,\n",
       "         -0.0106, -0.0467, -0.0352,  0.0306, -0.0792,  0.0619,  0.0940, -0.1300,\n",
       "         -0.0956,  0.0355, -0.0479,  0.0872,  0.2224,  0.0847,  0.0003,  0.0302,\n",
       "          0.0049,  0.1193,  0.0004,  0.1054, -0.1280,  0.0250, -0.1437, -0.0890,\n",
       "          0.0117, -0.0768,  0.0099,  0.0070,  0.0359,  0.1195, -0.0032,  0.1232,\n",
       "          0.2382,  0.1280, -0.0606, -0.1039, -0.1409, -0.0592, -0.0195, -0.0245,\n",
       "         -0.0230,  0.0321,  0.1016, -0.0485,  0.0795, -0.0363, -0.0146,  0.0939,\n",
       "         -0.1289, -0.1218,  0.0233, -0.0888, -0.0461, -0.0188,  0.0174,  0.0127,\n",
       "         -0.0934, -0.1272, -0.0103,  0.1083, -0.0706, -0.1350, -0.0529,  0.0209,\n",
       "         -0.0867, -0.0036,  0.1830, -0.0564, -0.0311,  0.0020, -0.0184,  0.0955,\n",
       "         -0.1504, -0.1452,  0.0459, -0.0443,  0.0312, -0.0061, -0.0063, -0.0699]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)\n",
    "print(contrast_feature.shape)\n",
    "contrast_feature\n",
    "# => we would like to look at all images in the batch size for the original and the transformed => 128 vectors for 6 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor_feature = contrast_feature\n",
    "anchor_count = contrast_count\n",
    "anchor_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 128])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 6]),\n",
       " tensor([[10.0001,  9.9973,  9.9975, 10.0000,  9.6229,  9.9787],\n",
       "         [ 9.9973,  9.9997,  9.9895,  9.9976,  9.5582,  9.9610],\n",
       "         [ 9.9975,  9.9895,  9.9999,  9.9968,  9.6816,  9.9908],\n",
       "         [10.0000,  9.9976,  9.9968,  9.9998,  9.6172,  9.9772],\n",
       "         [ 9.6229,  9.5582,  9.6816,  9.6172, 10.0000,  9.7801],\n",
       "         [ 9.9787,  9.9610,  9.9908,  9.9772,  9.7801, 10.0001]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor_dot_contrast = torch.div(\n",
    "            torch.matmul(anchor_feature, contrast_feature.T),\n",
    "            temperature)\n",
    "# in loss imi intra features (dim 128) generate de la 3 imagini odata, si fiecare feature vine si pt o a doua imagine (transform din prima)\n",
    "# asta inseamna ca ajung la un numar de 3x2 imagini pt care am features, de unde matrix 6x128\n",
    "\n",
    "# aici fac dot prod - contrast feature cu contrast feature transpus => calculam defapt cosine distances intre features pt cele 6 imagini, apoi impartim toata matricea la temp\n",
    "\n",
    "anchor_dot_contrast.shape, anchor_dot_contrast # ajungem la un matrix 6x6 pt pt fiecare feature; din outputul de mai jos deducem ca intre poza 0 si poza 3 din cele 6 exista sim ft mare etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10.0001],\n",
       "        [ 9.9997],\n",
       "        [ 9.9999],\n",
       "        [10.0000],\n",
       "        [10.0000],\n",
       "        [10.0001]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
    "logits_max\n",
    "# calculam similaritatile maxime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00, -2.8267e-03, -2.6684e-03, -1.9550e-04, -3.7729e-01,\n",
       "         -2.1453e-02],\n",
       "        [-2.3527e-03,  0.0000e+00, -1.0150e-02, -2.0914e-03, -4.4145e-01,\n",
       "         -3.8680e-02],\n",
       "        [-2.4195e-03, -1.0375e-02,  0.0000e+00, -3.0661e-03, -3.1827e-01,\n",
       "         -9.1209e-03],\n",
       "        [ 0.0000e+00, -2.3699e-03, -3.1195e-03, -1.5450e-04, -3.8274e-01,\n",
       "         -2.2764e-02],\n",
       "        [-3.7715e-01, -4.4178e-01, -3.1838e-01, -3.8280e-01,  0.0000e+00,\n",
       "         -2.1987e-01],\n",
       "        [-2.1361e-02, -3.9062e-02, -9.2783e-03, -2.2868e-02, -2.1992e-01,\n",
       "          0.0000e+00]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = anchor_dot_contrast - logits_max.detach()\n",
    "logits\n",
    "# aducem logits la valori mai mici scazand max-ul pt fiecare \"imagine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 1., 1., 0., 1.],\n",
       "        [0., 1., 0., 0., 1., 0.],\n",
       "        [1., 0., 1., 1., 0., 1.],\n",
       "        [1., 0., 1., 1., 0., 1.],\n",
       "        [0., 1., 0., 0., 1., 0.],\n",
       "        [1., 0., 1., 1., 0., 1.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = mask.repeat(anchor_count, contrast_count)\n",
    "mask\n",
    "# ne cream o masca care sub forma de 2x masca 2x masca (contrast count, anch count = 2) \n",
    "# avem 1 peste tot intre self-self si self-tr self\n",
    "# pt ca nu ne intereseaza sa ne uitam la self - self sims, dar nici la self- self transformed sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.arange(batch_size * anchor_count).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 1., 1., 1.],\n",
       "        [1., 0., 1., 1., 1., 1.],\n",
       "        [1., 1., 0., 1., 1., 1.],\n",
       "        [1., 1., 1., 0., 1., 1.],\n",
       "        [1., 1., 1., 1., 0., 1.],\n",
       "        [1., 1., 1., 1., 1., 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_mask = torch.scatter(\n",
    "            torch.ones_like(mask), # input 6x6 with 1\n",
    "            1, #dim\n",
    "            torch.arange(batch_size * anchor_count).view(-1, 1), #diagonala \n",
    "            0 # val pe diagonala\n",
    "        )\n",
    "logits_mask\n",
    "# create a mask with 1 everywehre instead of the diagonal (self-self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 1., 1., 0., 1.],\n",
       "        [0., 0., 0., 0., 1., 0.],\n",
       "        [1., 0., 0., 1., 0., 1.],\n",
       "        [1., 0., 1., 0., 0., 1.],\n",
       "        [0., 1., 0., 0., 0., 0.],\n",
       "        [1., 0., 1., 1., 0., 0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = mask * logits_mask\n",
    "mask\n",
    "# current mask forces us to only look at similarity between the anchor and the positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00, -2.8267e-03, -2.6684e-03, -1.9550e-04, -3.7729e-01,\n",
       "         -2.1453e-02],\n",
       "        [-2.3527e-03,  0.0000e+00, -1.0150e-02, -2.0914e-03, -4.4145e-01,\n",
       "         -3.8680e-02],\n",
       "        [-2.4195e-03, -1.0375e-02,  0.0000e+00, -3.0661e-03, -3.1827e-01,\n",
       "         -9.1209e-03],\n",
       "        [ 0.0000e+00, -2.3699e-03, -3.1195e-03, -1.5450e-04, -3.8274e-01,\n",
       "         -2.2764e-02],\n",
       "        [-3.7715e-01, -4.4178e-01, -3.1838e-01, -3.8280e-01,  0.0000e+00,\n",
       "         -2.1987e-01],\n",
       "        [-2.1361e-02, -3.9062e-02, -9.2783e-03, -2.2868e-02, -2.1992e-01,\n",
       "          0.0000e+00]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 1., 1., 1.],\n",
       "        [1., 0., 1., 1., 1., 1.],\n",
       "        [1., 1., 0., 1., 1., 1.],\n",
       "        [1., 1., 1., 0., 1., 1.],\n",
       "        [1., 1., 1., 1., 0., 1.],\n",
       "        [1., 1., 1., 1., 1., 0.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_mask"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "current step:\n",
    "- mask makes us look only between each anchor in the batch and its augmented positive or class positives and their augmentations (supcon) - numerator\n",
    "- logits mask makes us look for each anchor in the batch to everything else other than itself - denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0000, 0.9972, 0.9973, 0.9998, 0.6857, 0.9788],\n",
       "         [0.9977, 0.0000, 0.9899, 0.9979, 0.6431, 0.9621],\n",
       "         [0.9976, 0.9897, 0.0000, 0.9969, 0.7274, 0.9909],\n",
       "         [1.0000, 0.9976, 0.9969, 0.0000, 0.6820, 0.9775],\n",
       "         [0.6858, 0.6429, 0.7273, 0.6820, 0.0000, 0.8026],\n",
       "         [0.9789, 0.9617, 0.9908, 0.9774, 0.8026, 0.0000]]),\n",
       " tensor([[1.5388],\n",
       "         [1.5240],\n",
       "         [1.5481],\n",
       "         [1.5377],\n",
       "         [1.2643],\n",
       "         [1.5500]]),\n",
       " tensor([[-1.5388, -1.5416, -1.5414, -1.5390, -1.9161, -1.5602],\n",
       "         [-1.5264, -1.5240, -1.5342, -1.5261, -1.9655, -1.5627],\n",
       "         [-1.5505, -1.5585, -1.5481, -1.5512, -1.8664, -1.5572],\n",
       "         [-1.5377, -1.5401, -1.5408, -1.5379, -1.9205, -1.5605],\n",
       "         [-1.6414, -1.7061, -1.5827, -1.6471, -1.2643, -1.4842],\n",
       "         [-1.5713, -1.5890, -1.5592, -1.5728, -1.7699, -1.5500]]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_logits = torch.exp(logits) * logits_mask \n",
    "# of all feature similarities, we only consider those unmasked by logits mask \n",
    "# (i.e. all but self)\n",
    "log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True)) \n",
    "# here the mathematic formula had log e ^ dot prod anchor positive / sum of e ^ ...\n",
    "# in the code, the first term of the log product was taken out => dif of logs \n",
    "# e^ dot prod no longer computed because we eventually have a canceling out log (e ^ dor prod) = dot prod\n",
    "\n",
    "exp_logits, torch.log(exp_logits.sum(1, keepdim=True)), log_prob\n",
    "# results in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.9972, 0.9973, 0.9998, 0.6857, 0.9788],\n",
       "        [0.9977, 0.0000, 0.9899, 0.9979, 0.6431, 0.9621],\n",
       "        [0.9976, 0.9897, 0.0000, 0.9969, 0.7274, 0.9909],\n",
       "        [1.0000, 0.9976, 0.9969, 0.0000, 0.6820, 0.9775],\n",
       "        [0.6858, 0.6429, 0.7273, 0.6820, 0.0000, 0.8026],\n",
       "        [0.9789, 0.9617, 0.9908, 0.9774, 0.8026, 0.0000]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.9973, 0.9998, 0.0000, 0.9788],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.6431, 0.0000],\n",
       "        [0.9976, 0.0000, 0.0000, 0.9969, 0.0000, 0.9909],\n",
       "        [1.0000, 0.0000, 0.9969, 0.0000, 0.0000, 0.9775],\n",
       "        [0.0000, 0.6429, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.9789, 0.0000, 0.9908, 0.9774, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask * exp_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-4.6406, -1.9655, -4.6589, -4.6391, -1.7061, -4.7034])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(mask * log_prob).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.5469, -1.9655, -1.5530, -1.5464, -1.7061, -1.5678])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_log_prob_pos = (mask * log_prob).sum(1) / mask.sum(1) \n",
    "# sum of log probs just for positives / sum of all ; the -1 in the formula is included below\n",
    "# this is the outer sum from the formula\n",
    "mean_log_prob_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.2098, 2.8078, 2.2185, 2.2091, 2.4373, 2.2397])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = - (temperature / 0.07) * mean_log_prob_pos # regularization\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3537)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = loss.view(anchor_count, batch_size).mean()\n",
    "loss\n",
    "# because we work with a matrix, this will make sure we operate to all anchors in a batch \n",
    "# to get a loss per batch and not per anchor, we perform an avg "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hard Negatives"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an example to be hard negative, its loss must be very similar to the loss for a positive, but the label must be negative (very similar images to SQ but not SQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = [[0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1], [1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1], [1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1], [1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1], [1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1], [1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], [1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1], [1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = [[0.00000000e+00, 1.15979339e-04, 1.28603875e-04, 3.93253031e-05, 2.77834297e-05, 6.82534983e-06, 2.36547521e-06, 9.99240074e-05, 4.59065377e-05, 5.60972076e-05, 2.08943147e-05, 1.37771758e-05, 3.78358556e-04, 2.10736416e-05, 2.31846989e-05, 2.15018281e-05, 9.13694918e-01, 1.13968992e-04, 1.12911817e-04, 4.24974387e-05, 2.71834506e-05, 6.42993609e-06, 1.91244226e-06, 8.25789102e-05, 4.93223924e-05, 3.75325872e-05, 1.76978720e-05, 1.63649675e-05, 3.47876805e-04, 2.42568622e-05, 2.09306527e-05, 2.33844403e-05], [1.15979448e-04, 0.00000000e+00, 3.68726905e-05, 2.95725590e-06, 1.64781959e-05, 1.01547193e-05, 2.39604087e-05, 1.01512684e-04, 6.70701920e-05, 3.95469688e-05, 9.48735906e-05, 3.12319426e-05, 4.84115386e-04, 3.41254818e-05, 2.76728300e-04, 5.41074733e-05, 1.32441957e-04, 9.59271491e-01, 3.31144038e-05, 4.23505526e-06, 1.65883303e-05, 1.01689948e-05, 2.30238074e-05, 1.11849295e-04, 7.37373339e-05, 2.84794532e-05, 1.02510829e-04, 2.69782358e-05, 4.96094231e-04, 2.16127009e-05, 2.53369275e-04, 6.83679973e-05], [1.28604006e-04, 3.68726905e-05, 0.00000000e+00, 2.09297541e-05, 1.46303428e-05, 1.28054062e-05, 3.57001190e-05, 9.43750128e-05, 2.16380049e-05, 4.79159062e-05, 1.15107621e-04, 8.09703761e-06, 8.03803778e-05, 7.42521879e-05, 3.75995842e-05, 1.07842616e-05, 1.29919441e-04, 3.59358855e-05, 9.73823249e-01, 2.16012850e-05, 1.49200814e-05, 1.38452851e-05, 2.80195309e-05, 1.03151695e-04, 2.43056766e-05, 4.82075557e-05, 1.33507696e-04, 9.22044183e-06, 8.29584242e-05, 1.00744997e-04, 4.21278928e-05, 9.97927236e-06], [3.93253395e-05, 2.95725590e-06, 2.09297541e-05, 0.00000000e+00, 3.74953452e-05, 1.62687229e-05, 7.64006836e-05, 4.33622918e-05, 2.14661759e-05, 1.50535006e-05, 1.12709771e-04, 3.33960139e-04, 1.54607842e-05, 5.93666664e-06, 3.23892309e-05, 7.91521143e-06, 4.18997079e-05, 3.17946319e-06, 2.14944484e-05, 9.35800612e-01, 3.62490064e-05, 1.66570353e-05, 5.97332837e-05, 3.79038429e-05, 3.18036837e-05, 2.28904100e-05, 1.45898055e-04, 2.84253358e-04, 1.56039951e-05, 5.49761353e-06, 2.90853131e-05, 6.81952724e-06], [2.77834570e-05, 1.64781959e-05, 1.46303428e-05, 3.74953452e-05, 0.00000000e+00, 2.44453386e-05, 1.96298352e-05, 1.46901957e-05, 1.71459724e-05, 2.38175180e-05, 2.64675564e-05, 4.77042522e-05, 1.82159977e-06, 1.86327507e-05, 3.27011840e-05, 2.59253247e-05, 3.00489883e-05, 1.52245284e-05, 1.47388200e-05, 3.92300462e-05, 9.79579628e-01, 2.56963795e-05, 3.10969808e-05, 1.71836218e-05, 8.90483807e-06, 2.29325797e-05, 2.89456275e-05, 4.37435010e-05, 1.90189496e-06, 1.72441487e-05, 2.79505657e-05, 2.64937717e-05], [6.82536984e-06, 1.01547394e-05, 1.28054307e-05, 1.62687538e-05, 2.44453859e-05, 0.00000000e+00, 2.37484666e-04, 1.97972877e-05, 3.14780373e-05, 3.01237906e-05, 3.56397242e-04, 8.16898755e-05, 4.62244680e-05, 9.10423405e-05, 5.45534313e-05, 3.93303671e-05, 7.59222849e-06, 8.70421718e-06, 1.32718660e-05, 1.79189592e-05, 2.22148574e-05, 9.71428156e-01, 2.03052186e-04, 1.90135343e-05, 4.33957175e-05, 3.11240401e-05, 2.56918109e-04, 8.01202405e-05, 4.53063767e-05, 7.36195725e-05, 6.69492874e-05, 3.11449730e-05], [2.36547521e-06, 2.39603869e-05, 3.57000863e-05, 7.64006181e-05, 1.96298170e-05, 2.37483982e-04, 0.00000000e+00, 1.45895056e-05, 1.46440925e-05, 3.05296744e-05, 4.36049704e-05, 3.07127739e-05, 2.69764860e-05, 1.37986295e-04, 3.99230266e-05, 7.39711759e-05, 2.04529238e-06, 2.15707132e-05, 4.06040454e-05, 6.81545353e-05, 2.02138381e-05, 2.53972627e-04, 7.39541113e-01, 1.67038033e-05, 2.98078103e-05, 3.23569075e-05, 4.19059033e-05, 3.58992656e-05, 2.46350228e-05, 1.30246699e-04, 4.17912815e-05, 6.63771207e-05], [9.99241965e-05, 1.01512778e-04, 9.43750929e-05, 4.33623318e-05, 1.46901957e-05, 1.97972695e-05, 1.45895328e-05, 0.00000000e+00, 3.84207815e-05, 2.84851303e-05, 6.20410865e-05, 3.70488306e-05, 2.15407617e-05, 3.57634708e-05, 1.01629696e-04, 8.11685277e-06, 1.03740371e-04, 1.01246420e-04, 9.10085728e-05, 4.61737982e-05, 1.47470455e-05, 1.81351115e-05, 1.78821356e-05, 9.67499793e-01, 3.73530165e-05, 2.69326047e-05, 6.34364405e-05, 3.34828655e-05, 2.25284803e-05, 2.54683637e-05, 9.97213283e-05, 8.28930570e-06], [4.59065814e-05, 6.70701920e-05, 2.16380049e-05, 2.14661759e-05, 1.71459724e-05, 3.14779754e-05, 1.46441071e-05, 3.84207415e-05, 0.00000000e+00, 3.63030085e-05, 2.32734892e-05, 5.52279234e-05, 7.02659745e-05, 1.54191948e-04, 1.13627786e-04, 4.04837556e-05, 4.58633040e-05, 6.28500493e-05, 1.91250347e-05, 1.98553535e-05, 1.96733527e-05, 2.82699275e-05, 1.49589059e-05, 4.04760322e-05, 8.14396441e-01, 3.52198549e-05, 2.48953602e-05, 4.82933283e-05, 6.51418886e-05, 3.09998752e-04, 1.19443146e-04, 5.65269947e-05], [5.60973167e-05, 3.95470088e-05, 4.79159535e-05, 1.50535152e-05, 2.38175417e-05, 3.01237596e-05, 3.05297326e-05, 2.84851303e-05, 3.63030449e-05, 0.00000000e+00, 1.60636610e-05, 2.93708254e-05, 4.67689460e-05, 7.12899200e-05, 4.91569866e-04, 1.02666883e-04, 6.05478745e-05, 3.74372830e-05, 5.02039002e-05, 1.76806825e-05, 2.50381727e-05, 2.86776230e-05, 2.87175517e-05, 2.69843586e-05, 3.56642886e-05, 8.87629628e-01, 1.39399090e-05, 2.88357787e-05, 4.74387198e-05, 4.34159592e-05, 4.44840902e-04, 1.01475125e-04], [2.08943147e-05, 9.48734960e-05, 1.15107403e-04, 1.12709662e-04, 2.64675327e-05, 3.56396224e-04, 4.36049704e-05, 6.20409701e-05, 2.32734656e-05, 1.60636300e-05, 0.00000000e+00, 9.11233001e-05, 3.55845696e-04, 1.32340327e-04, 1.32984787e-04, 6.53384341e-05, 2.11778206e-05, 8.94631448e-05, 1.32372006e-04, 1.45233775e-04, 2.56185212e-05, 3.03725479e-04, 4.24205427e-05, 5.58432948e-05, 2.84033704e-05, 1.95338853e-05, 9.53724504e-01, 1.09788489e-04, 3.84419865e-04, 1.27199572e-04, 1.19423559e-04, 4.59819457e-05], [1.37772022e-05, 3.12319426e-05, 8.09705307e-06, 3.33960459e-04, 4.77042995e-05, 8.16897955e-05, 3.07128321e-05, 3.70488306e-05, 5.52279780e-05, 2.93708254e-05, 9.11234747e-05, 0.00000000e+00, 1.25687438e-04, 1.24650178e-05, 7.64763536e-05, 4.28405154e-04, 1.21836765e-05, 3.15217185e-05, 7.35326694e-06, 3.37175705e-04, 4.52617205e-05, 7.78175599e-05, 3.22859778e-05, 3.43917200e-05, 1.01359932e-04, 3.11738877e-05, 8.65797629e-05, 9.27420795e-01, 1.43919387e-04, 1.79719445e-05, 6.24617096e-05, 3.24145512e-04], [3.78358905e-04, 4.84115386e-04, 8.03803778e-05, 1.54607842e-05, 1.82159977e-06, 4.62243806e-05, 2.69765096e-05, 2.15407417e-05, 7.02659745e-05, 4.67689024e-05, 3.55846016e-04, 1.25687322e-04, 0.00000000e+00, 5.04925229e-05, 2.14533047e-05, 2.35372660e-04, 2.89460848e-04, 4.95175133e-04, 7.16122886e-05, 1.68968345e-05, 1.93755955e-06, 4.55101654e-05, 2.86301838e-05, 1.84386408e-05, 1.01615551e-04, 4.12523586e-05, 4.03175247e-04, 1.53451721e-04, 9.59810495e-01, 4.17239426e-05, 2.00500126e-05, 2.93597492e-04], [2.10736616e-05, 3.41254818e-05, 7.42521879e-05, 5.93666664e-06, 1.86327507e-05, 9.10421659e-05, 1.37986426e-04, 3.57634381e-05, 1.54191948e-04, 7.12898473e-05, 1.32340443e-04, 1.24650051e-05, 5.04925229e-05, 0.00000000e+00, 2.15410182e-04, 6.79317673e-05, 2.12350551e-05, 3.77176693e-05, 8.26669930e-05, 6.39352902e-06, 1.68154311e-05, 9.60251637e-05, 7.42811608e-05, 3.75487361e-05, 9.93707363e-05, 6.45231121e-05, 1.03219405e-04, 1.13987626e-05, 5.75772283e-05, 6.87526643e-01, 2.00506169e-04, 8.76721315e-05], [2.31847425e-05, 2.76728300e-04, 3.75996206e-05, 3.23892600e-05, 3.27012167e-05, 5.45533803e-05, 3.99231067e-05, 1.01629696e-04, 1.13627895e-04, 4.91569866e-04, 1.32985049e-04, 7.64763536e-05, 2.14533247e-05, 2.15410590e-04, 0.00000000e+00, 9.49947353e-05, 2.90326880e-05, 2.43634044e-04, 4.64034820e-05, 4.05653409e-05, 3.17433223e-05, 5.29553872e-05, 4.52799395e-05, 1.06138330e-04, 1.51918299e-04, 4.69248916e-04, 1.12890928e-04, 5.51096527e-05, 2.22900435e-05, 1.33042369e-04, 9.40959752e-01, 9.42741681e-05], [2.15018481e-05, 5.41074733e-05, 1.07842616e-05, 7.91521143e-06, 2.59253247e-05, 3.93302907e-05, 7.39712414e-05, 8.11684458e-06, 4.04837556e-05, 1.02666687e-04, 6.53384996e-05, 4.28404746e-04, 2.35372660e-04, 6.79317673e-05, 9.49946407e-05, 0.00000000e+00, 2.00613922e-05, 4.47855455e-05, 1.04673481e-05, 8.29189867e-06, 2.47530315e-05, 4.32220804e-05, 6.53139505e-05, 8.14801479e-06, 4.73104519e-05, 1.16982206e-04, 6.43724125e-05, 4.11739282e-04, 2.15683584e-04, 5.28375458e-05, 8.99801234e-05, 9.00041640e-01], [9.13695753e-01, 1.32441957e-04, 1.29919441e-04, 4.18997079e-05, 3.00489883e-05, 7.59221439e-06, 2.04529420e-06, 1.03740269e-04, 4.58633040e-05, 6.05478162e-05, 2.11778406e-05, 1.21836647e-05, 2.89460848e-04, 2.12350551e-05, 2.90326589e-05, 2.00613922e-05, 0.00000000e+00, 1.28117026e-04, 1.16689276e-04, 4.70859341e-05, 2.90278131e-05, 7.20789285e-06, 1.64212247e-06, 8.65232942e-05, 4.85925120e-05, 4.23495621e-05, 1.76314643e-05, 1.39337553e-05, 2.68322998e-04, 2.26897828e-05, 2.66250263e-05, 2.25083368e-05], [1.13969210e-04, 9.59271491e-01, 3.59358855e-05, 3.17946319e-06, 1.52245284e-05, 8.70420081e-06, 2.15707350e-05, 1.01246325e-04, 6.28500493e-05, 3.74372503e-05, 8.94632249e-05, 3.15216857e-05, 4.95175133e-04, 3.77176693e-05, 2.43633796e-04, 4.47855455e-05, 1.28117026e-04, 0.00000000e+00, 3.21748303e-05, 4.45748992e-06, 1.52366274e-05, 8.64355843e-06, 1.98214893e-05, 1.12003218e-04, 6.71400121e-05, 2.71990339e-05, 9.78563548e-05, 2.79008982e-05, 5.08794154e-04, 2.33138962e-05, 2.19440408e-04, 5.76901766e-05], [1.12911919e-04, 3.31144038e-05, 9.73823249e-01, 2.14944484e-05, 1.47388200e-05, 1.32718396e-05, 4.06040817e-05, 9.10084855e-05, 1.91250347e-05, 5.02038529e-05, 1.32372123e-04, 7.35325921e-06, 7.16122886e-05, 8.26669930e-05, 4.64034383e-05, 1.04673481e-05, 1.16689276e-04, 3.21748303e-05, 0.00000000e+00, 2.25557178e-05, 1.48140834e-05, 1.43975058e-05, 3.25357250e-05, 9.86141677e-05, 2.17094821e-05, 5.10316640e-05, 1.51341621e-04, 8.36996605e-06, 7.44043791e-05, 1.10352084e-04, 5.26378753e-05, 9.61398382e-06], [4.24975187e-05, 4.23505935e-06, 2.16013050e-05, 9.35801506e-01, 3.92300826e-05, 1.79189410e-05, 6.81546662e-05, 4.61737982e-05, 1.98553716e-05, 1.76806825e-05, 1.45234051e-04, 3.37175705e-04, 1.68968509e-05, 6.39353539e-06, 4.05653409e-05, 8.29190685e-06, 4.70859777e-05, 4.45749401e-06, 2.25557378e-05, 0.00000000e+00, 3.79738522e-05, 1.82132972e-05, 5.33350067e-05, 4.05596511e-05, 2.95763239e-05, 2.69288030e-05, 1.89563681e-04, 2.75796163e-04, 1.71313768e-05, 5.83592146e-06, 3.62321734e-05, 7.27691304e-06], [2.71834779e-05, 1.65883303e-05, 1.49200814e-05, 3.62490064e-05, 9.79579628e-01, 2.22148137e-05, 2.02138581e-05, 1.47470309e-05, 1.96733527e-05, 2.50381490e-05, 2.56185449e-05, 4.52616769e-05, 1.93755955e-06, 1.68154311e-05, 3.17432932e-05, 2.47530315e-05, 2.90278131e-05, 1.52366274e-05, 1.48140834e-05, 3.79738158e-05, 0.00000000e+00, 2.32685616e-05, 3.30310722e-05, 1.74911347e-05, 1.01432597e-05, 2.42875558e-05, 2.85102433e-05, 4.07658445e-05, 2.00283694e-06, 1.58332550e-05, 2.72789912e-05, 2.54928837e-05], [6.42994837e-06, 1.01690039e-05, 1.38452988e-05, 1.66570517e-05, 2.56964286e-05, 9.71427321e-01, 2.53973121e-04, 1.81351115e-05, 2.82699530e-05, 2.86776230e-05, 3.03726061e-04, 7.78175599e-05, 4.55102127e-05, 9.60252582e-05, 5.29553872e-05, 4.32221241e-05, 7.20789922e-06, 8.64356662e-06, 1.43975185e-05, 1.82132972e-05, 2.32686070e-05, 0.00000000e+00, 2.07189980e-04, 1.74429933e-05, 3.89717788e-05, 2.99481126e-05, 2.19571870e-04, 7.48169987e-05, 4.47063867e-05, 7.23489284e-05, 6.49517024e-05, 3.42889834e-05], [1.91244590e-06, 2.30238293e-05, 2.80195563e-05, 5.97333383e-05, 3.10970099e-05, 2.03051983e-04, 7.39542544e-01, 1.78821356e-05, 1.49589196e-05, 2.87175517e-05, 4.24206228e-05, 3.22859778e-05, 2.86302093e-05, 7.42812263e-05, 4.52799395e-05, 6.53140160e-05, 1.64212406e-06, 1.98215075e-05, 3.25357578e-05, 5.33350067e-05, 3.30311050e-05, 2.07189980e-04, 0.00000000e+00, 2.16295866e-05, 2.98540672e-05, 3.20397376e-05, 4.12385525e-05, 3.63902545e-05, 2.63675138e-05, 8.90020747e-05, 4.80063791e-05, 5.79185617e-05], [8.25789903e-05, 1.11849295e-04, 1.03151695e-04, 3.79038429e-05, 1.71836218e-05, 1.90134979e-05, 1.67038197e-05, 9.67498899e-01, 4.04760322e-05, 2.69843331e-05, 5.58433458e-05, 3.43916909e-05, 1.84386408e-05, 3.75487361e-05, 1.06138228e-04, 8.14801479e-06, 8.65232942e-05, 1.12003218e-04, 9.86141677e-05, 4.05596147e-05, 1.74911347e-05, 1.74429751e-05, 2.16295684e-05, 0.00000000e+00, 3.85877465e-05, 2.58682485e-05, 5.71908095e-05, 3.06131151e-05, 1.95316497e-05, 2.68759522e-05, 1.04581835e-04, 8.52484754e-06], [4.93224870e-05, 7.37373994e-05, 2.43056984e-05, 3.18037128e-05, 8.90485535e-06, 4.33956775e-05, 2.98078667e-05, 3.73530165e-05, 8.14397275e-01, 3.56642886e-05, 2.84034249e-05, 1.01359932e-04, 1.01615638e-04, 9.93708381e-05, 1.51918299e-04, 4.73104992e-05, 4.85925557e-05, 6.71400849e-05, 2.17095021e-05, 2.95763239e-05, 1.01432597e-05, 3.89717788e-05, 2.98540672e-05, 3.85877865e-05, 0.00000000e+00, 3.40131592e-05, 3.02661301e-05, 8.83519097e-05, 9.28402878e-05, 1.97188376e-04, 1.64071840e-04, 6.45337568e-05], [3.75326272e-05, 2.84794532e-05, 4.82075557e-05, 2.28904100e-05, 2.29325797e-05, 3.11239783e-05, 3.23569366e-05, 2.69325792e-05, 3.52198549e-05, 8.87628794e-01, 1.95339035e-05, 3.11738549e-05, 4.12523586e-05, 6.45231121e-05, 4.69248451e-04, 1.16982206e-04, 4.23495621e-05, 2.71990339e-05, 5.10316640e-05, 2.69287793e-05, 2.42875558e-05, 2.99480853e-05, 3.20397085e-05, 2.58682485e-05, 3.40131264e-05, 0.00000000e+00, 1.71618576e-05, 3.09210518e-05, 4.17649862e-05, 3.85495332e-05, 4.20214608e-04, 1.12794936e-04], [1.76978720e-05, 1.02510727e-04, 1.33507565e-04, 1.45897910e-04, 2.89456020e-05, 2.56917352e-04, 4.19059033e-05, 6.34363241e-05, 2.48953365e-05, 1.39398826e-05, 9.53724504e-01, 8.65795955e-05, 4.03174869e-04, 1.03219303e-04, 1.12890717e-04, 6.43723470e-05, 1.76314461e-05, 9.78562675e-05, 1.51341475e-04, 1.89563332e-04, 2.85102142e-05, 2.19571448e-04, 4.12384725e-05, 5.71907512e-05, 3.02660719e-05, 1.71618412e-05, 0.00000000e+00, 1.02651218e-04, 4.27161896e-04, 9.88874745e-05, 1.01131205e-04, 4.59784387e-05], [1.63649838e-05, 2.69782358e-05, 9.22044183e-06, 2.84253358e-04, 4.37435010e-05, 8.01200877e-05, 3.58993020e-05, 3.34828328e-05, 4.82933283e-05, 2.88357496e-05, 1.09788598e-04, 9.27419960e-01, 1.53451721e-04, 1.13987626e-05, 5.51095982e-05, 4.11739282e-04, 1.39337553e-05, 2.79008982e-05, 8.36996605e-06, 2.75795901e-04, 4.07658445e-05, 7.48169332e-05, 3.63902218e-05, 3.06131151e-05, 8.83518223e-05, 3.09210518e-05, 1.02651320e-04, 0.00000000e+00, 1.76158748e-04, 1.68052175e-05, 4.54313740e-05, 3.06876085e-04], [3.47877154e-04, 4.96094231e-04, 8.29584242e-05, 1.56039951e-05, 1.90189496e-06, 4.53062894e-05, 2.46350228e-05, 2.25284584e-05, 6.51418886e-05, 4.74386725e-05, 3.84420215e-04, 1.43919257e-04, 9.59810495e-01, 5.75772283e-05, 2.22900217e-05, 2.15683584e-04, 2.68322998e-04, 5.08794154e-04, 7.44043791e-05, 1.71313604e-05, 2.00283694e-06, 4.47063430e-05, 2.63674883e-05, 1.95316497e-05, 9.28402078e-05, 4.17649862e-05, 4.27162275e-04, 1.76158748e-04, 0.00000000e+00, 4.67899590e-05, 2.05585729e-05, 2.63786671e-04], [2.42568840e-05, 2.16127009e-05, 1.00744997e-04, 5.49761353e-06, 1.72441487e-05, 7.36194270e-05, 1.30246830e-04, 2.54683400e-05, 3.09998752e-04, 4.34159192e-05, 1.27199703e-04, 1.79719445e-05, 4.17239426e-05, 6.87526643e-01, 1.33042369e-04, 5.28375458e-05, 2.26897828e-05, 2.33138962e-05, 1.10352084e-04, 5.83591600e-06, 1.58332550e-05, 7.23488629e-05, 8.90019874e-05, 2.68759522e-05, 1.97188187e-04, 3.85495332e-05, 9.88875618e-05, 1.68052175e-05, 4.67899590e-05, 0.00000000e+00, 1.29460081e-04, 6.48956629e-05], [2.09306727e-05, 2.53369275e-04, 4.21278928e-05, 2.90853131e-05, 2.79505657e-05, 6.69491637e-05, 4.17913216e-05, 9.97212410e-05, 1.19443146e-04, 4.44840465e-04, 1.19423668e-04, 6.24616441e-05, 2.00500126e-05, 2.00506169e-04, 9.40958917e-01, 8.99801234e-05, 2.66250263e-05, 2.19440408e-04, 5.26378753e-05, 3.62321407e-05, 2.72789912e-05, 6.49516369e-05, 4.80063354e-05, 1.04581835e-04, 1.64071840e-04, 4.20214608e-04, 1.01131292e-04, 4.54313740e-05, 2.05585729e-05, 1.29460081e-04, 0.00000000e+00, 9.18962469e-05], [2.33844621e-05, 6.83679973e-05, 9.97927236e-06, 6.81952724e-06, 2.64937717e-05, 3.11449148e-05, 6.63771789e-05, 8.28930570e-06, 5.65269947e-05, 1.01475031e-04, 4.59819857e-05, 3.24145221e-04, 2.93597492e-04, 8.76721315e-05, 9.42740808e-05, 9.00041640e-01, 2.25083368e-05, 5.76901766e-05, 9.61398382e-06, 7.27690576e-06, 2.54928837e-05, 3.42889543e-05, 5.79185107e-05, 8.52484754e-06, 6.45336986e-05, 1.12794936e-04, 4.59784787e-05, 3.06876085e-04, 2.63786671e-04, 6.48956629e-05, 9.18962469e-05, 0.00000000e+00]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbs = [[0],[0],[1],[0],[1],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_probs = [[0.00000000e+00, 1.15979339e-04, 0.00000000e+00, 3.93253031e-05,  0.00000000e+00, 6.82534983e-06, 2.36547521e-06, 9.99240074e-05,  4.59065377e-05, 5.60972076e-05, 2.08943147e-05, 1.37771758e-05,  3.78358556e-04, 2.10736416e-05, 2.31846989e-05, 2.15018281e-05,  9.13694918e-01, 1.13968992e-04, 0.00000000e+00, 4.24974387e-05,  0.00000000e+00, 6.42993609e-06, 1.91244226e-06, 8.25789102e-05,  4.93223924e-05, 3.75325872e-05, 1.76978720e-05, 1.63649675e-05,  3.47876805e-04, 2.42568622e-05, 2.09306527e-05, 2.33844403e-05],  [1.15979448e-04, 0.00000000e+00, 0.00000000e+00, 2.95725590e-06,  0.00000000e+00, 1.01547193e-05, 2.39604087e-05, 1.01512684e-04,  6.70701920e-05, 3.95469688e-05, 9.48735906e-05, 3.12319426e-05,  4.84115386e-04, 3.41254818e-05, 2.76728300e-04, 5.41074733e-05,  1.32441957e-04, 9.59271491e-01, 0.00000000e+00, 4.23505526e-06,  0.00000000e+00, 1.01689948e-05, 2.30238074e-05, 1.11849295e-04,  7.37373339e-05, 2.84794532e-05, 1.02510829e-04, 2.69782358e-05,  4.96094231e-04, 2.16127009e-05, 2.53369275e-04, 6.83679973e-05],  [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,  1.46303428e-05, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,  0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,  0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,  0.00000000e+00, 0.00000000e+00, 9.73823249e-01, 0.00000000e+00,  1.49200814e-05, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,  0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,  0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],  [3.93253395e-05, 2.95725590e-06, 0.00000000e+00, 0.00000000e+00,  0.00000000e+00, 1.62687229e-05, 7.64006836e-05, 4.33622918e-05,  2.14661759e-05, 1.50535006e-05, 1.12709771e-04, 3.33960139e-04,  1.54607842e-05, 5.93666664e-06, 3.23892309e-05, 7.91521143e-06,  4.18997079e-05, 3.17946319e-06, 0.00000000e+00, 9.35800612e-01,  0.00000000e+00, 1.66570353e-05, 5.97332837e-05, 3.79038429e-05,  3.18036837e-05, 2.28904100e-05, 1.45898055e-04, 2.84253358e-04,  1.56039951e-05, 5.49761353e-06, 2.90853131e-05, 6.81952724e-06],  [0.00000000e+00, 0.00000000e+00, 1.46303428e-05, 0.00000000e+00,  0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,  0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,  0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,  0.00000000e+00, 0.00000000e+00, 1.47388200e-05, 0.00000000e+00,  9.79579628e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,  0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,  0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],  [6.82536984e-06, 1.01547394e-05, 0.00000000e+00, 1.62687538e-05,  0.00000000e+00, 0.00000000e+00, 2.37484666e-04, 1.97972877e-05,  3.14780373e-05, 3.01237906e-05, 3.56397242e-04, 8.16898755e-05,  4.62244680e-05, 9.10423405e-05, 5.45534313e-05, 3.93303671e-05,  7.59222849e-06, 8.70421718e-06, 0.00000000e+00, 1.79189592e-05,  0.00000000e+00, 9.71428156e-01, 2.03052186e-04, 1.90135343e-05,  4.33957175e-05, 3.11240401e-05, 2.56918109e-04, 8.01202405e-05,  4.53063767e-05, 7.36195725e-05, 6.69492874e-05, 3.11449730e-05],  [2.36547521e-06, 2.39603869e-05, 0.00000000e+00, 7.64006181e-05,  0.00000000e+00, 2.37483982e-04, 0.00000000e+00, 1.45895056e-05,  1.46440925e-05, 3.05296744e-05, 4.36049704e-05, 3.07127739e-05,  2.69764860e-05, 1.37986295e-04, 3.99230266e-05, 7.39711759e-05,  2.04529238e-06, 2.15707132e-05, 0.00000000e+00, 6.81545353e-05,  0.00000000e+00, 2.53972627e-04, 7.39541113e-01, 1.67038033e-05,  2.98078103e-05, 3.23569075e-05, 4.19059033e-05, 3.58992656e-05,  2.46350228e-05, 1.30246699e-04, 4.17912815e-05, 6.63771207e-05],  [9.99241965e-05, 1.01512778e-04, 0.00000000e+00, 4.33623318e-05,  0.00000000e+00, 1.97972695e-05, 1.45895328e-05, 0.00000000e+00,  3.84207815e-05, 2.84851303e-05, 6.20410865e-05, 3.70488306e-05,  2.15407617e-05, 3.57634708e-05, 1.01629696e-04, 8.11685277e-06,  1.03740371e-04, 1.01246420e-04, 0.00000000e+00, 4.61737982e-05,  0.00000000e+00, 1.81351115e-05, 1.78821356e-05, 9.67499793e-01,  3.73530165e-05, 2.69326047e-05, 6.34364405e-05, 3.34828655e-05,  2.25284803e-05, 2.54683637e-05, 9.97213283e-05, 8.28930570e-06],  [4.59065814e-05, 6.70701920e-05, 0.00000000e+00, 2.14661759e-05,  0.00000000e+00, 3.14779754e-05, 1.46441071e-05, 3.84207415e-05,  0.00000000e+00, 3.63030085e-05, 2.32734892e-05, 5.52279234e-05,  7.02659745e-05, 1.54191948e-04, 1.13627786e-04, 4.04837556e-05,  4.58633040e-05, 6.28500493e-05, 0.00000000e+00, 1.98553535e-05,  0.00000000e+00, 2.82699275e-05, 1.49589059e-05, 4.04760322e-05,  8.14396441e-01, 3.52198549e-05, 2.48953602e-05, 4.82933283e-05,  6.51418886e-05, 3.09998752e-04, 1.19443146e-04, 5.65269947e-05],  [5.60973167e-05, 3.95470088e-05, 0.00000000e+00, 1.50535152e-05,  0.00000000e+00, 3.01237596e-05, 3.05297326e-05, 2.84851303e-05,  3.63030449e-05, 0.00000000e+00, 1.60636610e-05, 2.93708254e-05,  4.67689460e-05, 7.12899200e-05, 4.91569866e-04, 1.02666883e-04,  6.05478745e-05, 3.74372830e-05, 0.00000000e+00, 1.76806825e-05,  0.00000000e+00, 2.86776230e-05, 2.87175517e-05, 2.69843586e-05,  3.56642886e-05, 8.87629628e-01, 1.39399090e-05, 2.88357787e-05,  4.74387198e-05, 4.34159592e-05, 4.44840902e-04, 1.01475125e-04],  [2.08943147e-05, 9.48734960e-05, 0.00000000e+00, 1.12709662e-04,  0.00000000e+00, 3.56396224e-04, 4.36049704e-05, 6.20409701e-05,  2.32734656e-05, 1.60636300e-05, 0.00000000e+00, 9.11233001e-05,  3.55845696e-04, 1.32340327e-04, 1.32984787e-04, 6.53384341e-05,  2.11778206e-05, 8.94631448e-05, 0.00000000e+00, 1.45233775e-04,  0.00000000e+00, 3.03725479e-04, 4.24205427e-05, 5.58432948e-05,  2.84033704e-05, 1.95338853e-05, 9.53724504e-01, 1.09788489e-04,  3.84419865e-04, 1.27199572e-04, 1.19423559e-04, 4.59819457e-05],  [1.37772022e-05, 3.12319426e-05, 0.00000000e+00, 3.33960459e-04,  0.00000000e+00, 8.16897955e-05, 3.07128321e-05, 3.70488306e-05,  5.52279780e-05, 2.93708254e-05, 9.11234747e-05, 0.00000000e+00,  1.25687438e-04, 1.24650178e-05, 7.64763536e-05, 4.28405154e-04,  1.21836765e-05, 3.15217185e-05, 0.00000000e+00, 3.37175705e-04,  0.00000000e+00, 7.78175599e-05, 3.22859778e-05, 3.43917200e-05,  1.01359932e-04, 3.11738877e-05, 8.65797629e-05, 9.27420795e-01,  1.43919387e-04, 1.79719445e-05, 6.24617096e-05, 3.24145512e-04],  [3.78358905e-04, 4.84115386e-04, 0.00000000e+00, 1.54607842e-05,  0.00000000e+00, 4.62243806e-05, 2.69765096e-05, 2.15407417e-05,  7.02659745e-05, 4.67689024e-05, 3.55846016e-04, 1.25687322e-04,  0.00000000e+00, 5.04925229e-05, 2.14533047e-05, 2.35372660e-04,  2.89460848e-04, 4.95175133e-04, 0.00000000e+00, 1.68968345e-05,  0.00000000e+00, 4.55101654e-05, 2.86301838e-05, 1.84386408e-05,  1.01615551e-04, 4.12523586e-05, 4.03175247e-04, 1.53451721e-04,  9.59810495e-01, 4.17239426e-05, 2.00500126e-05, 2.93597492e-04],  [2.10736616e-05, 3.41254818e-05, 0.00000000e+00, 5.93666664e-06,  0.00000000e+00, 9.10421659e-05, 1.37986426e-04, 3.57634381e-05,  1.54191948e-04, 7.12898473e-05, 1.32340443e-04, 1.24650051e-05,  5.04925229e-05, 0.00000000e+00, 2.15410182e-04, 6.79317673e-05,  2.12350551e-05, 3.77176693e-05, 0.00000000e+00, 6.39352902e-06,  0.00000000e+00, 9.60251637e-05, 7.42811608e-05, 3.75487361e-05,  9.93707363e-05, 6.45231121e-05, 1.03219405e-04, 1.13987626e-05,  5.75772283e-05, 6.87526643e-01, 2.00506169e-04, 8.76721315e-05],  [2.31847425e-05, 2.76728300e-04, 0.00000000e+00, 3.23892600e-05,  0.00000000e+00, 5.45533803e-05, 3.99231067e-05, 1.01629696e-04,  1.13627895e-04, 4.91569866e-04, 1.32985049e-04, 7.64763536e-05,  2.14533247e-05, 2.15410590e-04, 0.00000000e+00, 9.49947353e-05,  2.90326880e-05, 2.43634044e-04, 0.00000000e+00, 4.05653409e-05,  0.00000000e+00, 5.29553872e-05, 4.52799395e-05, 1.06138330e-04,  1.51918299e-04, 4.69248916e-04, 1.12890928e-04, 5.51096527e-05,  2.22900435e-05, 1.33042369e-04, 9.40959752e-01, 9.42741681e-05],  [2.15018481e-05, 5.41074733e-05, 0.00000000e+00, 7.91521143e-06,  0.00000000e+00, 3.93302907e-05, 7.39712414e-05, 8.11684458e-06,  4.04837556e-05, 1.02666687e-04, 6.53384996e-05, 4.28404746e-04,  2.35372660e-04, 6.79317673e-05, 9.49946407e-05, 0.00000000e+00,  2.00613922e-05, 4.47855455e-05, 0.00000000e+00, 8.29189867e-06,  0.00000000e+00, 4.32220804e-05, 6.53139505e-05, 8.14801479e-06,  4.73104519e-05, 1.16982206e-04, 6.43724125e-05, 4.11739282e-04,  2.15683584e-04, 5.28375458e-05, 8.99801234e-05, 9.00041640e-01],  [9.13695753e-01, 1.32441957e-04, 0.00000000e+00, 4.18997079e-05,  0.00000000e+00, 7.59221439e-06, 2.04529420e-06, 1.03740269e-04,  4.58633040e-05, 6.05478162e-05, 2.11778406e-05, 1.21836647e-05,  2.89460848e-04, 2.12350551e-05, 2.90326589e-05, 2.00613922e-05,  0.00000000e+00, 1.28117026e-04, 0.00000000e+00, 4.70859341e-05,  0.00000000e+00, 7.20789285e-06, 1.64212247e-06, 8.65232942e-05,  4.85925120e-05, 4.23495621e-05, 1.76314643e-05, 1.39337553e-05,  2.68322998e-04, 2.26897828e-05, 2.66250263e-05, 2.25083368e-05],  [1.13969210e-04, 9.59271491e-01, 0.00000000e+00, 3.17946319e-06,  0.00000000e+00, 8.70420081e-06, 2.15707350e-05, 1.01246325e-04,  6.28500493e-05, 3.74372503e-05, 8.94632249e-05, 3.15216857e-05,  4.95175133e-04, 3.77176693e-05, 2.43633796e-04, 4.47855455e-05,  1.28117026e-04, 0.00000000e+00, 0.00000000e+00, 4.45748992e-06,  0.00000000e+00, 8.64355843e-06, 1.98214893e-05, 1.12003218e-04,  6.71400121e-05, 2.71990339e-05, 9.78563548e-05, 2.79008982e-05,  5.08794154e-04, 2.33138962e-05, 2.19440408e-04, 5.76901766e-05],  [0.00000000e+00, 0.00000000e+00, 9.73823249e-01, 0.00000000e+00,  1.47388200e-05, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,  0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,  0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,  0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,  1.48140834e-05, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,  0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,  0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],  [4.24975187e-05, 4.23505935e-06, 0.00000000e+00, 9.35801506e-01,  0.00000000e+00, 1.79189410e-05, 6.81546662e-05, 4.61737982e-05,  1.98553716e-05, 1.76806825e-05, 1.45234051e-04, 3.37175705e-04,  1.68968509e-05, 6.39353539e-06, 4.05653409e-05, 8.29190685e-06,  4.70859777e-05, 4.45749401e-06, 0.00000000e+00, 0.00000000e+00,  0.00000000e+00, 1.82132972e-05, 5.33350067e-05, 4.05596511e-05,  2.95763239e-05, 2.69288030e-05, 1.89563681e-04, 2.75796163e-04,  1.71313768e-05, 5.83592146e-06, 3.62321734e-05, 7.27691304e-06],  [0.00000000e+00, 0.00000000e+00, 1.49200814e-05, 0.00000000e+00,  9.79579628e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,  0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,  0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,  0.00000000e+00, 0.00000000e+00, 1.48140834e-05, 0.00000000e+00,  0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,  0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,  0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],  [6.42994837e-06, 1.01690039e-05, 0.00000000e+00, 1.66570517e-05,  0.00000000e+00, 9.71427321e-01, 2.53973121e-04, 1.81351115e-05,  2.82699530e-05, 2.86776230e-05, 3.03726061e-04, 7.78175599e-05,  4.55102127e-05, 9.60252582e-05, 5.29553872e-05, 4.32221241e-05,  7.20789922e-06, 8.64356662e-06, 0.00000000e+00, 1.82132972e-05,  0.00000000e+00, 0.00000000e+00, 2.07189980e-04, 1.74429933e-05,  3.89717788e-05, 2.99481126e-05, 2.19571870e-04, 7.48169987e-05,  4.47063867e-05, 7.23489284e-05, 6.49517024e-05, 3.42889834e-05],  [1.91244590e-06, 2.30238293e-05, 0.00000000e+00, 5.97333383e-05,  0.00000000e+00, 2.03051983e-04, 7.39542544e-01, 1.78821356e-05,  1.49589196e-05, 2.87175517e-05, 4.24206228e-05, 3.22859778e-05,  2.86302093e-05, 7.42812263e-05, 4.52799395e-05, 6.53140160e-05,  1.64212406e-06, 1.98215075e-05, 0.00000000e+00, 5.33350067e-05,  0.00000000e+00, 2.07189980e-04, 0.00000000e+00, 2.16295866e-05,  2.98540672e-05, 3.20397376e-05, 4.12385525e-05, 3.63902545e-05,  2.63675138e-05, 8.90020747e-05, 4.80063791e-05, 5.79185617e-05],  [8.25789903e-05, 1.11849295e-04, 0.00000000e+00, 3.79038429e-05,  0.00000000e+00, 1.90134979e-05, 1.67038197e-05, 9.67498899e-01,  4.04760322e-05, 2.69843331e-05, 5.58433458e-05, 3.43916909e-05,  1.84386408e-05, 3.75487361e-05, 1.06138228e-04, 8.14801479e-06,  8.65232942e-05, 1.12003218e-04, 0.00000000e+00, 4.05596147e-05,  0.00000000e+00, 1.74429751e-05, 2.16295684e-05, 0.00000000e+00,  3.85877465e-05, 2.58682485e-05, 5.71908095e-05, 3.06131151e-05,  1.95316497e-05, 2.68759522e-05, 1.04581835e-04, 8.52484754e-06],  [4.93224870e-05, 7.37373994e-05, 0.00000000e+00, 3.18037128e-05,  0.00000000e+00, 4.33956775e-05, 2.98078667e-05, 3.73530165e-05,  8.14397275e-01, 3.56642886e-05, 2.84034249e-05, 1.01359932e-04,  1.01615638e-04, 9.93708381e-05, 1.51918299e-04, 4.73104992e-05,  4.85925557e-05, 6.71400849e-05, 0.00000000e+00, 2.95763239e-05,  0.00000000e+00, 3.89717788e-05, 2.98540672e-05, 3.85877865e-05,  0.00000000e+00, 3.40131592e-05, 3.02661301e-05, 8.83519097e-05,  9.28402878e-05, 1.97188376e-04, 1.64071840e-04, 6.45337568e-05],  [3.75326272e-05, 2.84794532e-05, 0.00000000e+00, 2.28904100e-05,  0.00000000e+00, 3.11239783e-05, 3.23569366e-05, 2.69325792e-05,  3.52198549e-05, 8.87628794e-01, 1.95339035e-05, 3.11738549e-05,  4.12523586e-05, 6.45231121e-05, 4.69248451e-04, 1.16982206e-04,  4.23495621e-05, 2.71990339e-05, 0.00000000e+00, 2.69287793e-05,  0.00000000e+00, 2.99480853e-05, 3.20397085e-05, 2.58682485e-05,  3.40131264e-05, 0.00000000e+00, 1.71618576e-05, 3.09210518e-05,  4.17649862e-05, 3.85495332e-05, 4.20214608e-04, 1.12794936e-04],  [1.76978720e-05, 1.02510727e-04, 0.00000000e+00, 1.45897910e-04,  0.00000000e+00, 2.56917352e-04, 4.19059033e-05, 6.34363241e-05,  2.48953365e-05, 1.39398826e-05, 9.53724504e-01, 8.65795955e-05,  4.03174869e-04, 1.03219303e-04, 1.12890717e-04, 6.43723470e-05,  1.76314461e-05, 9.78562675e-05, 0.00000000e+00, 1.89563332e-04,  0.00000000e+00, 2.19571448e-04, 4.12384725e-05, 5.71907512e-05,  3.02660719e-05, 1.71618412e-05, 0.00000000e+00, 1.02651218e-04,  4.27161896e-04, 9.88874745e-05, 1.01131205e-04, 4.59784387e-05],  [1.63649838e-05, 2.69782358e-05, 0.00000000e+00, 2.84253358e-04,  0.00000000e+00, 8.01200877e-05, 3.58993020e-05, 3.34828328e-05,  4.82933283e-05, 2.88357496e-05, 1.09788598e-04, 9.27419960e-01,  1.53451721e-04, 1.13987626e-05, 5.51095982e-05, 4.11739282e-04,  1.39337553e-05, 2.79008982e-05, 0.00000000e+00, 2.75795901e-04,  0.00000000e+00, 7.48169332e-05, 3.63902218e-05, 3.06131151e-05,  8.83518223e-05, 3.09210518e-05, 1.02651320e-04, 0.00000000e+00,  1.76158748e-04, 1.68052175e-05, 4.54313740e-05, 3.06876085e-04],  [3.47877154e-04, 4.96094231e-04, 0.00000000e+00, 1.56039951e-05,  0.00000000e+00, 4.53062894e-05, 2.46350228e-05, 2.25284584e-05,  6.51418886e-05, 4.74386725e-05, 3.84420215e-04, 1.43919257e-04,  9.59810495e-01, 5.75772283e-05, 2.22900217e-05, 2.15683584e-04,  2.68322998e-04, 5.08794154e-04, 0.00000000e+00, 1.71313604e-05,  0.00000000e+00, 4.47063430e-05, 2.63674883e-05, 1.95316497e-05,  9.28402078e-05, 4.17649862e-05, 4.27162275e-04, 1.76158748e-04,  0.00000000e+00, 4.67899590e-05, 2.05585729e-05, 2.63786671e-04],  [2.42568840e-05, 2.16127009e-05, 0.00000000e+00, 5.49761353e-06,  0.00000000e+00, 7.36194270e-05, 1.30246830e-04, 2.54683400e-05,  3.09998752e-04, 4.34159192e-05, 1.27199703e-04, 1.79719445e-05,  4.17239426e-05, 6.87526643e-01, 1.33042369e-04, 5.28375458e-05,  2.26897828e-05, 2.33138962e-05, 0.00000000e+00, 5.83591600e-06,  0.00000000e+00, 7.23488629e-05, 8.90019874e-05, 2.68759522e-05,  1.97188187e-04, 3.85495332e-05, 9.88875618e-05, 1.68052175e-05,  4.67899590e-05, 0.00000000e+00, 1.29460081e-04, 6.48956629e-05],  [2.09306727e-05, 2.53369275e-04, 0.00000000e+00, 2.90853131e-05,  0.00000000e+00, 6.69491637e-05, 4.17913216e-05, 9.97212410e-05,  1.19443146e-04, 4.44840465e-04, 1.19423668e-04, 6.24616441e-05,  2.00500126e-05, 2.00506169e-04, 9.40958917e-01, 8.99801234e-05,  2.66250263e-05, 2.19440408e-04, 0.00000000e+00, 3.62321407e-05,  0.00000000e+00, 6.49516369e-05, 4.80063354e-05, 1.04581835e-04,  1.64071840e-04, 4.20214608e-04, 1.01131292e-04, 4.54313740e-05,  2.05585729e-05, 1.29460081e-04, 0.00000000e+00, 9.18962469e-05],  [2.33844621e-05, 6.83679973e-05, 0.00000000e+00, 6.81952724e-06,  0.00000000e+00, 3.11449148e-05, 6.63771789e-05, 8.28930570e-06,  5.65269947e-05, 1.01475031e-04, 4.59819857e-05, 3.24145221e-04,  2.93597492e-04, 8.76721315e-05, 9.42740808e-05, 9.00041640e-01,  2.25083368e-05, 5.76901766e-05, 0.00000000e+00, 7.27690576e-06,  0.00000000e+00, 3.42889543e-05, 5.79185107e-05, 8.52484754e-06,  6.45336986e-05, 1.12794936e-04, 4.59784787e-05, 3.06876085e-04,  2.63786671e-04, 6.48956629e-05, 9.18962469e-05, 0.00000000e+00]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = np.array(mask)\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_probs = np.array(masked_probs)\n",
    "masked_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = np.array(probs)\n",
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.array(np.array(lbs) * np.array(lbs).T)\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total events: 53\n",
      "Train positives: 3954\n",
      "Train negatives: 10327\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(14281,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataset_3_ch import MyDataset\n",
    "import numpy as np \n",
    "\n",
    "train_dataset = MyDataset(data_dir=\"/home/vanessa/Dev/DATASETS/C23_C24_pos-upsampled-concat/\", \n",
    "                                    split=\"train\", \n",
    "                                    transform=None)\n",
    "\n",
    "targets = train_dataset.labels # 0 1 0 0 1\n",
    "class_sample_count = np.unique(targets, return_counts=True)[1]\n",
    "weight =  1. / class_sample_count # [0.0001, 0.001]\n",
    "samples_weight = weight[targets]\n",
    "samples_weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data={'mask' : mask[2], 'prob' : probs[2], 'mask_prob': masked_probs[2],'label' : labels[2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mask</th>\n",
       "      <th>prob</th>\n",
       "      <th>mask_prob</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>0.973823</td>\n",
       "      <td>0.973823</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mask      prob  mask_prob  label\n",
       "0      0  0.000129   0.000000      0\n",
       "1      0  0.000037   0.000000      0\n",
       "2      0  0.000000   0.000000      1\n",
       "3      0  0.000021   0.000000      0\n",
       "4      1  0.000015   0.000015      1\n",
       "5      0  0.000013   0.000000      0\n",
       "6      0  0.000036   0.000000      0\n",
       "7      0  0.000094   0.000000      0\n",
       "8      0  0.000022   0.000000      0\n",
       "9      0  0.000048   0.000000      0\n",
       "10     0  0.000115   0.000000      0\n",
       "11     0  0.000008   0.000000      0\n",
       "12     0  0.000080   0.000000      0\n",
       "13     0  0.000074   0.000000      0\n",
       "14     0  0.000038   0.000000      0\n",
       "15     0  0.000011   0.000000      0\n",
       "16     0  0.000130   0.000000      0\n",
       "17     0  0.000036   0.000000      0\n",
       "18     1  0.973823   0.973823      0\n",
       "19     0  0.000022   0.000000      0\n",
       "20     1  0.000015   0.000015      0\n",
       "21     0  0.000014   0.000000      0\n",
       "22     0  0.000028   0.000000      0\n",
       "23     0  0.000103   0.000000      0\n",
       "24     0  0.000024   0.000000      0\n",
       "25     0  0.000048   0.000000      0\n",
       "26     0  0.000134   0.000000      0\n",
       "27     0  0.000009   0.000000      0\n",
       "28     0  0.000083   0.000000      0\n",
       "29     0  0.000101   0.000000      0\n",
       "30     0  0.000042   0.000000      0\n",
       "31     0  0.000010   0.000000      0"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9189677724709364"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.power([0.9999],845)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2997039128017544"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.power([0.9999],)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0012340768981590972, 0.000155285761988641)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "beta = 0.9999\n",
    "\n",
    "wp = (1 - beta)/ (1- np.power([beta],845)[0])\n",
    "wn = (1 - beta)/ (1- np.power([beta],10327)[0])\n",
    "\n",
    "wp,wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "810.3222752907251"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/wp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6439.740432050357"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.000306182411089746, 0.000155285761988641)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "beta = 0.9999\n",
    "\n",
    "wp = (1 - beta)/ (1- np.power([beta],3954)[0])\n",
    "wn = (1 - beta)/ (1- np.power([beta],10327)[0])\n",
    "\n",
    "wp,wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.99999999999991"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "efp =  (1- np.power([beta],3954)[0]) / (1- beta)\n",
    "efp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6439.7404320503565"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "efn =  (1- np.power([beta],10327)[0]) / (1- beta)\n",
    "efn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0012340768981590972, 0.0001377235031318902)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "beta = 0.9999\n",
    "\n",
    "wp = (1 - beta)/ (1- np.power([beta],845)[0])\n",
    "wn = (1 - beta)/ (1- np.power([beta],12949)[0])\n",
    "\n",
    "wp,wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.26239470782443e-255, 4.26239470782443e-255)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.pow(beta, 845), np.power([beta],845)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "810.3222752907251"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "efp =  (1- np.power([beta],845)[0]) / (1- beta)\n",
    "efp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7260.924804115352"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "efp =  (1- np.power([beta],12949)[0]) / (1- beta)\n",
    "efp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels.float()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss code for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.9902103>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = tf.shape(features)[0]\n",
    "contrast_count = 1\n",
    "anchor_count = contrast_count\n",
    "y = tf.expand_dims(labels, -1)\n",
    "\n",
    "# mask: contrastive mask of shape [bsz, bsz], mask_{i,j}=1 if sample j\n",
    "#     has the same class as sample i. Can be asymmetric.\n",
    "mask = tf.cast(tf.equal(y, tf.transpose(y)), tf.float32)\n",
    "anchor_dot_contrast = tf.divide(\n",
    "    tf.matmul(labels, tf.transpose(labels)),\n",
    "    temperature\n",
    ")\n",
    "# # for numerical stability\n",
    "logits_max = tf.reduce_max(anchor_dot_contrast, axis=1, keepdims=True)\n",
    "logits = anchor_dot_contrast - logits_max\n",
    "# # tile mask\n",
    "logits_mask = tf.ones_like(mask) - tf.eye(batch_size)\n",
    "mask = mask * logits_mask\n",
    "# compute log_prob\n",
    "exp_logits = tf.exp(logits) * logits_mask\n",
    "log_prob = logits - \\\n",
    "    tf.math.log(tf.reduce_sum(exp_logits, axis=1, keepdims=True))\n",
    "\n",
    "# compute mean of log-likelihood over positive\n",
    "# this may introduce NaNs due to zero division,\n",
    "# when a class only has one example in the batch\n",
    "mask_sum = tf.reduce_sum(mask, axis=1)\n",
    "mean_log_prob_pos = tf.reduce_sum(\n",
    "    mask * log_prob, axis=1)[mask_sum > 0] / mask_sum[mask_sum > 0]\n",
    "\n",
    "# loss\n",
    "loss = -(temperature / 0.07) * mean_log_prob_pos\n",
    "# loss = tf.reduce_mean(tf.reshape(loss, [anchor_count, batch_size]))\n",
    "loss = tf.reduce_mean(loss)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.9902103>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
