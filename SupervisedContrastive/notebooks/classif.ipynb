{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier as knn\n",
    "from sklearn.ensemble import BaggingClassifier as knn_bagging\n",
    "from sklearn.linear_model import SGDClassifier as sgd\n",
    "from sklearn.linear_model import LogisticRegression as logistic_reg\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "# \n",
    "import imblearn\n",
    "plt.style.use(\"dark_background\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_run = False\n",
    "\n",
    "has_tr_train = True\n",
    "has_tr_test = False\n",
    "has_tr_unc = False\n",
    "\n",
    "use_uncertain = True\n",
    "encoding_size = 512\n",
    "\n",
    "run_on_raw = True\n",
    "knn_neighbors = 11\n",
    "\n",
    "run_name = 'C23_C24_july_12_rerun'#'c23_june_21' #'c23_c24_june_14'\n",
    "based_on = '12_07_redo-train/test/val' #'finetuning_14_06_23_c23_c24_no_prevs_replacement_pos_train/test_lowdecay_20.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./out/shuffled_train_val.csv')\n",
    "df_test = pd.read_csv('./out/shuffled_test.csv')\n",
    "\n",
    "df_test['name']= df_test['name'].str.replace('pos-concat-val','pos-concat')\n",
    "df_train['name']= df_train['name'].str.replace('pos-upsampled-concat','pos-concat')\n",
    "df_test['name']= df_test['name'].str.replace('pos-upsampled-concat','pos-concat')\n",
    "df_test['name']= df_test['name'].str.replace('dataset/C','dataset/all/C')\n",
    "df_train = df_train[~df_train['name'].isin(df_test['name'])]\n",
    "\n",
    "if use_uncertain:\n",
    "    df_uncertain_test = pd.read_csv('./out/shuffled_test.csv')\n",
    "\n",
    "# df_test = pd.concat([pd.read_csv('./negs_test.csv'), pd.read_csv('./out/07_07_upsfix_supcon_upsample_test.csv')], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/vanessa/Dev/DATASETS/C23_C24_pos-concat/poz/img_poz_1996_07_09_09_01_00_79.png'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['name'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['name'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if old_run:\n",
    "    cols_to_extract_no_tr  = ['useless1', 'part_of_quake','Year','month','day','event_idx','frame']\n",
    "else:\n",
    "    cols_to_extract_no_tr  = ['useless1','useless2', 'useless3', 'part_of_quake','Year','month','day','Hour','Minute','Seconds','frame']\n",
    "cols_to_extract_has_tr  = cols_to_extract_no_tr + ['transform']\n",
    "emb_cols = [f'{x}' for x in range(0,encoding_size)]\n",
    "# emb_cols = ['1', '3', '5', '7', '9', '11', '13', '15', '17']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_cols(df, has_tr):\n",
    "    if (has_tr):\n",
    "        df[cols_to_extract_has_tr] = df['name'].str.split('_', expand=True)\n",
    "        df.loc[df['transform'].isna(),'transform'] = '20'\n",
    "        df['transform'] = df['transform'].str.extract('(\\d+)', expand=False)\n",
    "        df['transform']=df['transform'].astype(int)\n",
    "    else:\n",
    "        df[cols_to_extract_no_tr] = df['name'].str.split('_', expand=True)\n",
    "    \n",
    "    df['frame'] = df['frame'].str.extract('(\\d+)', expand=False)\n",
    "    df['frame']=df['frame'].astype(int)\n",
    "    return df\n",
    "\n",
    "def set_custom_preds(df, predcol=''):\n",
    "    is_pos = 1 if df[predcol+'_preds'].mean() > 0.5 else 0\n",
    "    is_pos_any = 1 if df[predcol+'_preds'].any() else 0\n",
    "    df[f'avg_{predcol}_preds'] = is_pos\n",
    "    df[f'any_{predcol}_preds'] = is_pos_any\n",
    "    return df\n",
    "\n",
    "def set_preds(df_train, df_test, df_uncertain, preds_train, preds_test, preds_uncertain, clstype):\n",
    "    df_train[clstype+'_preds'] = preds_train\n",
    "    df_test[clstype+'_preds'] = preds_test\n",
    "    if use_uncertain:\n",
    "        df_uncertain[clstype+'_preds'] = preds_uncertain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = update_cols(df_train, has_tr=has_tr_train)\n",
    "df_test = update_cols(df_test, has_tr=has_tr_test)\n",
    "if use_uncertain:\n",
    "    df_uncertain_test = update_cols(df_uncertain_test, has_tr=has_tr_unc)\n",
    "else:\n",
    "    df_uncertain_test = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_train[(df_train['label'] == 1) & (df_train['transform'] == 20)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c23: 11 train 4 test\n",
    "\n",
    "\n",
    "c24: 25 train 7 test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not old_run:\n",
    "    df_train['date'] = pd.to_datetime(df_train[['Year', 'month', 'day', 'Hour','Minute','Seconds']])\n",
    "    df_train['date'].unique() \n",
    "    \n",
    "    df_test['date'] = pd.to_datetime(df_test[['Year', 'month', 'day', 'Hour','Minute','Seconds']])\n",
    "    df_test['date'].unique()\n",
    "\n",
    "    if use_uncertain:\n",
    "        df_uncertain_test['date'] = pd.to_datetime(df_uncertain_test[['Year', 'month', 'day', 'Hour','Minute','Seconds']])\n",
    "        df_uncertain_test['date'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_on_raw:\n",
    "    if (has_tr_train):\n",
    "        df_train = df_train[df_train['transform'] == 20]\n",
    "    if (has_tr_test):\n",
    "        df_test = df_test[df_test['transform'] == 20]\n",
    "    if use_uncertain:\n",
    "        if has_tr_unc:\n",
    "            df_uncertain_test = df_uncertain_test[df_uncertain_test['transform'] == 20]\n",
    "\n",
    "if (not has_tr_train and not has_tr_test):\n",
    "    run_on_raw = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = df_train[df_train['Year'] >= '2011']\n",
    "# df_test = df_test[df_test['Year'] >= '2011']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_train = scaler.fit_transform(df_train[emb_cols])\n",
    "scaled_test = scaler.transform(df_test[emb_cols])\n",
    "if use_uncertain:\n",
    "    scaled_uncertain = scaler.transform(df_uncertain_test[emb_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "over = SMOTE(sampling_strategy=0.2)\n",
    "under = RandomUnderSampler(sampling_strategy=0.75)\n",
    "\n",
    "steps = [('o', over), ('u', under)]\n",
    "pipeline = Pipeline(steps=steps)\n",
    "\n",
    "X, y = pipeline.fit_resample(scaled_train, df_train['label'])\n",
    "print (len([x for i,x in enumerate(y) if x == 0]))\n",
    "print (len([x for i,x in enumerate(y) if x == 1]))\n",
    "\n",
    "svc1 = SVC(C=0.005, kernel='poly')#, class_weight='balanced')\n",
    "svc1.fit(X, y)\n",
    "preds_train_lr = svc1.predict(scaled_train)\n",
    "preds_test_lr = svc1.predict(scaled_test)\n",
    "\n",
    "preds_unc_lr = None\n",
    "if use_uncertain:\n",
    "    preds_unc_lr = svc1.predict(scaled_uncertain)\n",
    "\n",
    "preds_y = svc1.predict(X)\n",
    "\n",
    "set_preds(df_train, df_test, df_uncertain_test, preds_train_lr, preds_test_lr, preds_unc_lr, f'logistic_regression')\n",
    "\n",
    "\n",
    "print(classification_report(df_test['label'], preds_test_lr))\n",
    "print(confusion_matrix(df_test['label'], preds_test_lr))\n",
    "if use_uncertain:\n",
    "    print(confusion_matrix(df_uncertain_test['label'], preds_unc_lr))\n",
    "    print(classification_report(df_uncertain_test['label'], preds_unc_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svc1 = SVC(C=0.005, kernel='poly')#, class_weight='balanced')\n",
    "# svc1.fit(scaled_train, df_train['label'])\n",
    "# preds_train_lr = svc1.predict(scaled_train)\n",
    "# preds_test_lr = svc1.predict(scaled_test)\n",
    "\n",
    "# preds_unc_lr = None\n",
    "# if use_uncertain:\n",
    "#     preds_unc_lr = svc1.predict(scaled_uncertain)\n",
    "\n",
    "# preds_y = svc1.predict(X)\n",
    "\n",
    "# set_preds(df_train, df_test, df_uncertain_test, preds_train_lr, preds_test_lr, preds_unc_lr, f'logistic_regression')\n",
    "\n",
    "\n",
    "# print(classification_report(df_test['label'], preds_test_lr))\n",
    "# print(confusion_matrix(df_test['label'], preds_test_lr))\n",
    "# if use_uncertain:\n",
    "#     print(confusion_matrix(df_uncertain_test['label'], preds_unc_lr))\n",
    "#     print(classification_report(df_uncertain_test['label'], preds_unc_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gb2 = gb_cols.remove('frame')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df_test['consec_pred_count'] = \n",
    "\n",
    "# def set_len(df):\n",
    "#     df['consec_pred_count'] = len(df)\n",
    "#     return df\n",
    "\n",
    "# df_test = df_test.sort_values(by=gb_cols+['frame'])\n",
    "# consecutives = df_test['logistic_regression_preds'].diff().ne(0).cumsum()\n",
    "# df_test = df_test.groupby(consecutives).apply(lambda df: set_len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test['final_preds'] = df_test['logistic_regression_preds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test.loc[(df_test['logistic_regression_preds'] == 1) & (df_test['consec_pred_count'] < 7),'final_preds'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test[(df_test['final_preds'] > 0) & (df_test['label'] > 0)][gb_cols+['frame','consec_pred_count']].sort_values(by=['Year','month','frame'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(confusion_matrix(df_test['label'], df_test['final_preds']))\n",
    "# print(classification_report(df_test['label'], df_test['final_preds']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(confusion_matrix(df_test['label'], df_test['logistic_regression_preds']))\n",
    "# print(classification_report(df_test['label'], df_test['logistic_regression_preds']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fp_unc = df_uncertain_test[(df_uncertain_test['logistic_regression_preds']==1) & (df_uncertain_test['label'] == 0)]\n",
    "# # g1 = fp_unc.groupby(['Year','month','day','Hour','Minute','Seconds'])\n",
    "# for i,g in enumerate(g1.groups):\n",
    "#     g1.get_group(g)\n",
    "#     print(f\"event: {'_'.join(str(i) for i in g)}, frames:\")\n",
    "#     print(grs[i].sort_values(by='frame')['frame'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fp = df_test[(df_test['logistic_regression_preds']==1) & (df_test['label'] == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g1 = fp.groupby(['Year','month','day','Hour','Minute','Seconds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,g in enumerate(g1.groups):\n",
    "#     g1.get_group(g)\n",
    "#     print(f\"event: {'_'.join(str(i) for i in g)}, frames:\")\n",
    "#     print(grs[i].sort_values(by='frame')['frame'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = model.predict(X)\n",
    "# print(confusion_matrix(y, p))\n",
    "# print(classification_report(y, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "\n",
    "# svc1 = SVC(C=0.5, kernel='poly')#, class_weight='balanced')\n",
    "# svc1 = svc1.fit(X, y)\n",
    "\n",
    "# # t = df_test[df_test['transform'] == 20]\n",
    "# # sctt = scaler.transform(t[emb_cols])\n",
    "\n",
    "# preds_train = svc1.predict(scaled_train)\n",
    "# preds = svc1.predict(scaled_test)#sctt)\n",
    "\n",
    "# print(classification_report(df_test['label'], preds))\n",
    "# print(confusion_matrix(df_test['label'], preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k in range(11,12):\n",
    "model = knn(n_neighbors=knn_neighbors)\n",
    "model.fit(X, y)\n",
    "\n",
    "preds_train = model.predict(scaled_train)\n",
    "preds = model.predict(scaled_test)\n",
    "\n",
    "uncertain_preds = None\n",
    "if use_uncertain:\n",
    "    uncertain_preds = model.predict(scaled_uncertain)\n",
    "\n",
    "set_preds(df_train, df_test, df_uncertain_test, preds_train, preds, uncertain_preds, f'knn_{knn_neighbors}')\n",
    "\n",
    "print(confusion_matrix(df_test['label'], df_test['knn_11_preds']))\n",
    "print(classification_report(df_test['label'], df_test['knn_11_preds']))\n",
    "\n",
    "if use_uncertain:\n",
    "    print(confusion_matrix(df_uncertain_test['label'], df_uncertain_test['knn_11_preds']))\n",
    "    print(classification_report(df_uncertain_test['label'], df_uncertain_test['knn_11_preds']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = knn_bagging(bootstrap=True,warm_start=True,bootstrap_features=True, max_features=1)\n",
    "model.fit(X, y)\n",
    "\n",
    "preds_train = model.predict(scaled_train)\n",
    "preds = model.predict(scaled_test)\n",
    "\n",
    "uncertain_preds = None\n",
    "if use_uncertain:\n",
    "    uncertain_preds = model.predict(scaled_uncertain)\n",
    "    \n",
    "set_preds(df_train, df_test, df_uncertain_test, preds_train, preds, uncertain_preds, f'knn_bagging')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # set_preds(df_train, df_test, df_uncertain_test, preds_train, preds, uncertain_preds, f'knn_{knn_neighbors}')\n",
    "# # print(confusion_matrix(df_train['label'], preds_train))\n",
    "# # print(classification_report(df_train['label'], preds_train))\n",
    "\n",
    "# # set_preds(df_train, df_test, df_uncertain_test, preds_train, preds, uncertain_preds, f'knn_{knn_neighbors}')\n",
    "# print(confusion_matrix(df_test['label'], preds))\n",
    "# print(classification_report(df_test['label'], preds))\n",
    "\n",
    "# print(\"--------------------UNC --------------\")\n",
    "\n",
    "# print(confusion_matrix(df_uncertain_test['label'], uncertain_preds))\n",
    "# print(classification_report(df_uncertain_test['label'], uncertain_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "kernels = ['poly', 'rbf']\n",
    "\n",
    "for i, kernel in enumerate(kernels):\n",
    "    svc1 = SVC(C=0.005, kernel=kernel)#, class_weight='balanced')\n",
    "    svc1.fit(X, y)\n",
    "\n",
    "    preds_train_svc = svc1.predict(scaled_train)\n",
    "    preds_test_svc = svc1.predict(scaled_test)\n",
    "\n",
    "    preds_unc_svc = None\n",
    "    if use_uncertain:\n",
    "        preds_unc_svc = model.predict(scaled_uncertain)\n",
    "\n",
    "    set_preds(df_train, df_test, df_uncertain_test, preds_train_svc, preds_test_svc, preds_unc_svc, f'svc_{kernel}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_test[df_test['date'].isin(df_train['date'].tolist())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(df_test['label'], df_test['svc_poly_preds']))\n",
    "print(classification_report(df_test['label'], df_test['svc_poly_preds']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(df_uncertain_test['label'], df_uncertain_test['svc_poly_preds']))\n",
    "print(classification_report(df_uncertain_test['label'], df_uncertain_test['svc_poly_preds']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(df_train['label'], df_train['svc_poly_preds']))\n",
    "print(classification_report(df_train['label'], df_train['svc_poly_preds']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(confusion_matrix(df_uncertain_test['label'], df_uncertain_test['svc_poly_preds']))\n",
    "print(classification_report(df_uncertain_test['label'], df_uncertain_test['svc_poly_preds']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fp = df_test[(df_test['svc_poly_preds']==1) & (df_test['label'] == 0)]\n",
    "# g1 = fp .groupby(['Year','month','day','Hour','Minute','Seconds'])\n",
    "# for i,g in enumerate(g1.groups):\n",
    "#     gr = g1.get_group(g)\n",
    "#     print(f\"event: {'_'.join(str(i) for i in g)}, frames:\")\n",
    "#     print(gr.sort_values(by='frame')['frame'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = logistic_reg()#max_iter=200, warm_start=True, tol=1e-1,solver='newton-cg')#class_weight='balanced')\n",
    "lr.fit(X, y)\n",
    "preds_train_lr = lr.predict(scaled_train)\n",
    "preds_test_lr = lr.predict(scaled_test)\n",
    "\n",
    "preds_unc_lr = None\n",
    "if use_uncertain:\n",
    "    preds_unc_lr = model.predict(scaled_uncertain)\n",
    "\n",
    "set_preds(df_train, df_test, df_uncertain_test, preds_train_lr, preds_test_lr, preds_unc_lr, f'logistic_regression')\n",
    "print(classification_report(df_test['label'], preds_test_lr))\n",
    "print(confusion_matrix(df_test['label'], preds_test_lr))\n",
    "\n",
    "print(classification_report(df_train['label'], preds_train_lr))\n",
    "print(confusion_matrix(df_train['label'], preds_train_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sgd(class_weight='balanced')\n",
    "model.fit(X, y)\n",
    "\n",
    "preds_train = model.predict(scaled_train)\n",
    "preds = model.predict(scaled_test)\n",
    "\n",
    "uncertain_preds = None\n",
    "if use_uncertain:\n",
    "    uncertain_preds = model.predict(scaled_uncertain)\n",
    "\n",
    "set_preds(df_train, df_test, df_uncertain_test, preds_train, preds, uncertain_preds, f'sgd')\n",
    "\n",
    "print(classification_report(df_test['label'],preds))\n",
    "print(confusion_matrix(df_test['label'],preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avg & Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if old_run:\n",
    "    gb_cols = ['part_of_quake','Year','month','day','event_idx','frame']\n",
    "else:   \n",
    "    gb_cols = ['part_of_quake','Year','month','day','Hour','Minute','Seconds','frame']\n",
    "\n",
    "cls_types = [f'knn_{knn_neighbors}', 'knn_bagging', 'svc_poly', 'svc_rbf','logistic_regression','sgd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if has_tr_train and has_tr_test and not run_on_raw:\n",
    "    for i, cls in enumerate(cls_types):\n",
    "        df_train = df_train.groupby(gb_cols).apply(lambda df: set_custom_preds(df,cls))\n",
    "        df_test = df_test.groupby(gb_cols).apply(lambda df: set_custom_preds(df,cls))\n",
    "        if use_uncertain:\n",
    "            df_uncertain_test = df_uncertain_test.groupby(gb_cols).apply(lambda df: set_custom_preds(df, cls))\n",
    "    cls_types = cls_types \n",
    "    cls_types_any_avg = [f'any_{classtype}' for i, classtype in enumerate(cls_types)] + [f'avg_{classtype}' for i, classtype in enumerate(cls_types)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(df_test['label'],df_test['logistic_regression_preds']))\n",
    "print(classification_report(df_test['label'],df_test['logistic_regression_preds']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(f'./out/builtpreds/{run_name}_train.csv',index=False)\n",
    "df_test.to_csv(f'./out/builtpreds/{run_name}_test.csv',index=False)\n",
    "\n",
    "if (use_uncertain):\n",
    "    df_uncertain_test.to_csv(f'./out/builtpreds/{run_name}_uncertain.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if has_tr_train:\n",
    "    df_train_raw = df_train[df_train['transform'] == 20]\n",
    "if  has_tr_test:\n",
    "    df_test_raw = df_test[df_test['transform'] == 20]\n",
    "if use_uncertain and has_tr_unc:\n",
    "    df_uncertain_raw = df_uncertain_test[df_uncertain_test['transform'] == 20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_uncertain:\n",
    "    print(\"\\nUncertain:\\n\")\n",
    "    print(classification_report(df_uncertain_test['label'], df_uncertain_test[f'svc_poly_preds']))\n",
    "    print((confusion_matrix(df_uncertain_test['label'], df_uncertain_test[f'svc_poly_preds'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uncertain_test[df_uncertain_test['svc_poly_preds']==1][['date','frame']].sort_values(by=['date','frame']).to_csv(\"false_positives_uncertain_12_july_smote.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uncertain_test[(df_uncertain_test['svc_poly_preds']==1) & (df_uncertain_test['label']==1)][['date','frame']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "with open(f\"results_all_contrastive_{run_name}{'_raw' if run_on_raw else ''}.txt\", 'w') as f:\n",
    "    f.write(f\"transforms train: {'true' if has_tr_train else 'false'}\\n\")\n",
    "    f.write(f\"transforms test: {'true' if has_tr_test else 'false'}\\n\")\n",
    "    if use_uncertain:\n",
    "        f.write(f\"transforms unc: {'true' if has_tr_unc else 'false'}\\n\")\n",
    "    \n",
    "\n",
    "    for i, cls in enumerate(cls_types):\n",
    "        f.write(f\"\\n------{cls} {'raw' if run_on_raw else ''} based on {based_on}------\\n\")\n",
    " \n",
    "        f.write(\"\\nTrain:\\n\")\n",
    "        f.write(classification_report(df_train['label'], df_train[f'{cls}_preds']))\n",
    "        f.write(np.array2string(confusion_matrix(df_train['label'], df_train[f'{cls}_preds'])))\n",
    "        f.write(\"\\nTest:\\n\")\n",
    "        f.write(classification_report(df_test['label'], df_test[f'{cls}_preds']))\n",
    "        f.write(np.array2string(confusion_matrix(df_test['label'], df_test[f'{cls}_preds'])))\n",
    "        if use_uncertain:\n",
    "            f.write(\"\\nUncertain:\\n\")\n",
    "            f.write(classification_report(df_uncertain_test['label'], df_uncertain_test[f'{cls}_preds']))\n",
    "            f.write(np.array2string(confusion_matrix(df_uncertain_test['label'], df_uncertain_test[f'{cls}_preds'])))\n",
    "\n",
    "    if not run_on_raw:\n",
    "        for i, cls in enumerate(cls_types_any_avg):\n",
    "            f.write(f\"\\n------{cls} {'raw' if run_on_raw else ''} based on {based_on}------\\n\")\n",
    " \n",
    "            f.write(\"\\nTrain:\\n\")\n",
    "            f.write(classification_report(df_train_raw['label'], df_train_raw[f'{cls}_preds']))\n",
    "            f.write(np.array2string(confusion_matrix(df_train_raw['label'], df_train_raw[f'{cls}_preds'])))\n",
    "            f.write(\"\\nTest:\\n\")\n",
    "            f.write(classification_report(df_test_raw['label'], df_test_raw[f'{cls}_preds']))\n",
    "            f.write(np.array2string(confusion_matrix(df_test_raw['label'], df_test_raw[f'{cls}_preds'])))\n",
    "            if use_uncertain:\n",
    "                f.write(\"\\nUncertain:\\n\")\n",
    "                f.write(classification_report(df_uncertain_raw['label'], df_uncertain_raw[f'{cls}_preds']))\n",
    "                f.write(np.array2string(confusion_matrix(df_uncertain_raw['label'], df_uncertain_raw[f'{cls}_preds'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_avg_cols(df):\n",
    "    for col in range(encoding_size):\n",
    "        df[f'avg_{col}'] = (df[df['transform']!=20][f'{col}'].mean() + df[df['transform']==20][f'{col}'].mean())/2\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_cp = df_train.copy()\n",
    "# x_cp = x_cp.groupby(gb_cols).apply(lambda df: set_avg_cols(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_cp = df_test.copy()\n",
    "x_test_cp = x_test_cp.groupby(gb_cols).apply(lambda df: set_avg_cols(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 5 7 9 10 13 15 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_cp[x_test_cp['avg_19'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "x_cp_raw = x_cp[x_cp['transform']==20]\n",
    "x_test_cp_raw = x_test_cp[x_test_cp['transform']==20]\n",
    "\n",
    "avg_emb_cols = [f'avg_{x}' for x in range(encoding_size)]\n",
    "\n",
    "scaled_train = scaler.fit_transform(x_cp_raw[avg_emb_cols])\n",
    "scaled_test = scaler.transform(x_test_cp_raw[avg_emb_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "over = SMOTE(sampling_strategy=0.1)\n",
    "under = RandomUnderSampler(sampling_strategy=0.5)\n",
    "\n",
    "steps = [('o', over), ('u', under)]\n",
    "pipeline = Pipeline(steps=steps)\n",
    "\n",
    "X, y = pipeline.fit_resample(scaled_train, x_cp_raw['label'])\n",
    "print (len([x for i,x in enumerate(y) if x == 0]))\n",
    "print (len([x for i,x in enumerate(y) if x == 1]))\n",
    "\n",
    "# for k in range(11,12):\n",
    "model = knn(n_neighbors=3)\n",
    "model.fit(X, y)\n",
    "\n",
    "preds_train = model.predict(scaled_train)\n",
    "preds = model.predict(scaled_test)\n",
    "\n",
    "# set_preds(df_train, df_test, df_uncertain_test, preds_train, preds, uncertain_preds, f'knn_{knn_neighbors}')\n",
    "print(confusion_matrix(x_test_cp_raw['label'], preds))\n",
    "print(classification_report(x_test_cp_raw['label'], preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"------------------------AVG CERTAIN:------------------------\")\n",
    "# print(classification_report(df_test['label'], df_test['avg_preds']))\n",
    "# print(confusion_matrix(df_test['label'], df_test['avg_preds']))\n",
    "\n",
    "# print(\"------------------------AVG UNCERTAIN:------------------------\")\n",
    "# print(classification_report(df_uncertain_test['label'], df_uncertain_test['avg_preds']))\n",
    "# print(confusion_matrix(df_uncertain_test['label'], df_uncertain_test['avg_preds']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"------------------------ANY CERTAIN:------------------------\")\n",
    "# print(classification_report(df_test['label'], df_test['any_preds']))\n",
    "# print(confusion_matrix(df_test['label'], df_test['any_preds']))\n",
    "\n",
    "# print(\"------------------------ANY UNCERTAIN:------------------------\")\n",
    "# print(classification_report(df_uncertain_test['label'], df_uncertain_test['any_preds']))\n",
    "# print(confusion_matrix(df_uncertain_test['label'], df_uncertain_test['any_preds']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"------------------------ANY RAW CERTAIN:------------------------\")\n",
    "# print(classification_report(df_test_raw['label'], df_test_raw['any_preds']))\n",
    "# print(confusion_matrix(df_test_raw['label'], df_test_raw['any_preds']))\n",
    "\n",
    "# print(\"------------------------ANY RAW UNCERTAIN:------------------------\")\n",
    "# print(classification_report(df_uncertain_raw['label'], df_uncertain_raw['any_preds']))\n",
    "# print(confusion_matrix(df_uncertain_raw['label'], df_uncertain_raw['any_preds']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test[(df_test['any_preds']==1) & (df_test['label']==1) & (df_test['Year']=='1996')].sort_values(by=['transform','frame'])[['transform','label','preds','any_preds','avg_preds','frame']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test_raw[(df_test_raw['any_preds']==1) & (df_test_raw['label']==0)].sort_values(by=['Year','month','frame'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_pos = df_test[(df_test['preds']==1) & (df_test['label']==1)]['name'].tolist()\n",
    "# pred_pos.sort()\n",
    "# # pred_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_wrong = df_test_raw[(df_test_raw['preds']==1) & (df_test_raw['label']==0)].sort_values(by=['Year','month','frame'])['name'].tolist()\n",
    "# pred_wrong"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False positives (detects SQ where there is none)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_wrong_uncertain = df_uncertain_raw[(df_uncertain_raw['any_preds']==1) & (df_uncertain_raw['label']==0)].sort_values(by=['Year','month','frame'])[['Year', 'month','day','Hour','Minute','Seconds','frame']]\n",
    "pred_wrong_uncertain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False negatives (misses to detect SQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_wrong_uncertain = df_uncertain_raw[(df_uncertain_raw['any_preds']==0) & (df_uncertain_raw['label']==1)].sort_values(by=['Year','month','frame'])[['Year', 'month','day','Hour','Minute','Seconds','frame']]\n",
    "pred_wrong_uncertain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True positives (detects SQ correctly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_wrong_uncertain = df_uncertain_raw[(df_uncertain_raw['any_preds']==1) & (df_uncertain_raw['label']==1)].sort_values(by=['Year','month','frame'])[['Year', 'month','day','Hour','Minute','Seconds','frame']]\n",
    "pred_wrong_uncertain"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
